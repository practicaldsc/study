<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Gradient Descent and Convexity</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../assets/theme.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Gradient Descent and Convexity</h1>
</header>
<p><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
<!-- add after bootstrap.min.css -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<!-- add after bootstrap.min.js or bootstrap.bundle.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script></p>
<!-- for difficulty gauges-->
<script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-B947E6J6H4"></script> -->
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  // gtag('js', new Date());

  // gtag('config', 'G-B947E6J6H4');
</script>
<p><a href="../index.html">← return to study.practicaldsc.org</a></p>
<hr />
<p>The problems in this worksheet are taken from past exams in similar
classes. Work on them <strong>on paper</strong>, since the exams you
take in this course will also be on paper. <br><br>We encourage you to
complete this worksheet in a live discussion section. Solutions will be
made available after all discussion sections have concluded. You don’t
need to submit your answers anywhere.<br><br><b>Note: We do not plan to
cover all problems here in the live discussion section</b>; the problems
we don’t cover can be used for extra practice.</p>
<hr />
<h2 id="problem-1">Problem 1</h2>
<p>Consider the least squares regression model, <span
class="math inline">\vec{h} = X \vec{w}</span>. Assume that <span
class="math inline">X</span> and <span
class="math inline">\vec{h}</span> refer to the design matrix and
hypothesis vector for our training data, and <span
class="math inline">\vec y</span> is the true observation vector.</p>
<p>Let <span class="math inline">\vec{w}_\text{OLS}^*</span> be the
parameter vector that minimizes mean squared error without
regularization. Specifically:</p>
<p><span class="math inline">\vec{w}_\text{OLS}^*</span> = <span
class="math inline">\arg\underset{\vec{w}}{\min} \frac{1}{n} \| \vec{y}
- X \vec{w} \|^2_2</span></p>
<p>Let <span class="math inline">\vec{w}_\text{ridge}^*</span> be the
parameter vector that minimizes mean squared error with <span
class="math inline">L_2</span> regularization, using a non-negative
regularization hyperparameter <span class="math inline">\lambda</span>
(i.e. ridge regression). Specifically:</p>
<p><span class="math inline">\vec{w}_\text{ridge}^*</span> = <span
class="math inline">\arg\underset{\vec{w}}{\min} \frac{1}{n} \| \vec y -
X \vec{w} \|^2_2 + \lambda \sum_{j=1}^{p} w_j^2</span></p>
<p>For each of the following problems, fill in the blank.</p>
<p><br></p>
<h3 id="problem-1.1">Problem 1.1</h3>
<p>If we set <span class="math inline">\lambda</span> = 0, then <span
class="math inline">\Vert \vec{w}_\text{OLS}^* \Vert^2_2</span> is
<strong>____</strong> <span class="math inline">\Vert
\vec{w}_\text{ridge}^* \Vert^2_2</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> less than</p></li>
<li><p><input type="radio" disabled="" /> equal to</p></li>
<li><p><input type="radio" disabled="" /> greater than</p></li>
<li><p><input type="radio" disabled="" /> impossible to tell</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.2">Problem 1.2</h3>
<p>For each of the remaining parts, you can assume that <span
class="math inline">\lambda</span> is set such that the predicted
response vectors for our two models (<span class="math inline">\vec{h} =
X \vec{w}_\text{OLS}^*</span> and <span class="math inline">\vec{h} = X
\vec{w}_\text{ridge}^*</span>) is different.</p>
<p>The <strong>training</strong> MSE of the model <span
class="math inline">\vec{h} = X \vec{w}_\text{OLS}^*</span> is
<strong>____</strong> than the model <span class="math inline">\vec{h} =
X \vec{w}_\text{ridge}^*</span>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> less than</p></li>
<li><p><input type="radio" disabled="" /> equal to</p></li>
<li><p><input type="radio" disabled="" /> greater than</p></li>
<li><p><input type="radio" disabled="" /> impossible to tell</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.3">Problem 1.3</h3>
<p>Now, assume we’ve fit both models using our training data, and
evaluate both models on some unseen testing data.</p>
<p>The <strong>test</strong> MSE of the model <span
class="math inline">\vec{h} = X \vec{w}_\text{OLS}^*</span> is
<strong>____</strong> than the model <span class="math inline">\vec{h} =
X \vec{w}_\text{ridge}^*</span>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> less than</p></li>
<li><p><input type="radio" disabled="" /> equal to</p></li>
<li><p><input type="radio" disabled="" /> greater than</p></li>
<li><p><input type="radio" disabled="" /> impossible to tell</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.4">Problem 1.4</h3>
<p>Assume that our design matrix <span class="math inline">X</span>
contains a column of all ones. The sum of the residuals of our model
<span class="math inline">\vec{h} = X \vec{w}_\text{ridge}^*</span>
<strong>____</strong>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> equal to 0</p></li>
<li><p><input type="radio" disabled="" /> not necessarily equal to 0</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.5">Problem 1.5</h3>
<p>As we increase <span class="math inline">\lambda</span>, the bias of
the model <span class="math inline">\vec{h} = X
\vec{w}_\text{ridge}^*</span> tends to <strong>____</strong>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> increase</p></li>
<li><p><input type="radio" disabled="" /> stay the same</p></li>
<li><p><input type="radio" disabled="" /> decrease</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.6">Problem 1.6</h3>
<p>As we increase <span class="math inline">\lambda</span>, the model
variance of the model <span class="math inline">\vec{h} = X
\vec{w}_\text{ridge}^*</span> tends to <strong>____</strong>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> increase</p></li>
<li><p><input type="radio" disabled="" /> stay the same</p></li>
<li><p><input type="radio" disabled="" /> decrease</p></li>
</ul>
<p><br></p>
<h3 id="problem-1.7">Problem 1.7</h3>
<p>As we increase <span class="math inline">\lambda</span>, the
observation variance of the model <span class="math inline">\vec{h} = X
\vec{w}_\text{ridge}^*</span> tends to <strong>____</strong>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> increase</p></li>
<li><p><input type="radio" disabled="" /> stay the same</p></li>
<li><p><input type="radio" disabled="" /> decrease</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-2">Problem 2</h2>
<p>Let <span class="math inline">\vec{x} = \begin{bmatrix} x_1 \\ x_2
\end{bmatrix}</span>. Consider the function <span
class="math inline">g(\vec{x}) = (x_1 - 3)^2 + (x_1^2 -
x_2)^2</span>.</p>
<p><br></p>
<h3 id="problem-2.1">Problem 2.1</h3>
<p>Find <span class="math inline">\nabla g(\vec{x})</span>, the gradient
of <span class="math inline">g(\vec{x})</span>, and use it to show that
<span class="math inline">\nabla g\left( \begin{bmatrix} -1 \\ 1
\end{bmatrix} \right) = \begin{bmatrix} -8 \\ 0
\end{bmatrix}</span>.</p>
<p><br></p>
<h3 id="problem-2.2">Problem 2.2</h3>
<p>We’d like to find the vector <span
class="math inline">\vec{x}^*</span> that minimizes <span
class="math inline">g(\vec{x})</span> using gradient descent. Perform
one iteration of gradient descent by hand, using the initial guess <span
class="math inline">\vec{x}^{(0)} = \begin{bmatrix} -1 \\ 1
\end{bmatrix}</span> and the learning rate <span
class="math inline">\alpha = \frac{1}{2}</span>. In other words, what is
<span class="math inline">\vec{x}^{(1)}</span>?</p>
<p><br></p>
<h3 id="problem-2.3">Problem 2.3</h3>
<p>Consider the function <span class="math inline">f(t) = (t - 3)^2 +
(t^2 - 1)^2</span>. Select the true statement below.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">f(t)</span> is convex and has a global
minimum.</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">f(t)</span> is not convex, but has a global
minimum.</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">f(t)</span> is convex, but doesn’t have a
global minimum.</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">f(t)</span> is not convex and doesn’t have
a global minimum.</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-3">Problem 3</h2>
<p>Remember to show your work and justify your answers.</p>
<p>Suppose we want to minimize the function</p>
<p><span class="math display">R(h) = e^{(h + 1)^2}</span></p>
<p><br></p>
<h3 id="problem-3.1">Problem 3.1</h3>
<p>Without using gradient descent or calculus, what is the value <span
class="math inline">h^*</span> that minimizes <span
class="math inline">R(h)</span>?</p>
<p><br></p>
<h3 id="problem-3.2">Problem 3.2</h3>
<p>Now, suppose we want to use gradient descent to minimize <span
class="math inline">R(h)</span>. Assume we use an initial guess of <span
class="math inline">h_0 = 0</span>. What is <span
class="math inline">h_1</span>? Give your answer in terms of a generic
step size, <span class="math inline">\alpha</span>, and other constants.
(<span class="math inline">e</span> is a constant.)</p>
<p><br></p>
<h3 id="problem-3.3">Problem 3.3</h3>
<p>Using your answers from the previous two parts, what should we set
the value of <span class="math inline">\alpha</span> to be if we want to
ensure that gradient descent finds <span class="math inline">h^*</span>
after just one iteration?</p>
<p><br></p>
<h3 id="problem-3.4">Problem 3.4</h3>
<p>Below is a graph of <span class="math inline">R(h)</span> with no
axis labels.</p>
<!-- TODO -->
<center><img src="../assets/images/disc11/graph.png" width="500" height="350"></center>
<p>True or False: Given an appropriate choice of step size, <span
class="math inline">\alpha</span>, gradient descent is guaranteed to
find the minimizer of <span class="math inline">R(h)</span>.</p>
<p><br></p>
<hr />
<h2 id="problem-4">Problem 4</h2>
<!-- Convexity Problem -->
Let <span class="math inline">f(x):\mathbb{R}\to\mathbb{R}</span> be a
convex function. <span class="math inline">f(x)</span> is not
necessarily differentiable. Use the definition of convexity to prove the
following:
<span class="math display">\begin{aligned}
            2f(2) \leq f(1)+f(3)
\end{aligned}</span>
<hr />
<h2 id="problem-5">Problem 5</h2>
<p>Consider the function <span class="math inline">R(h) = \sqrt{(h -
3)^2 + 1} = ((h - 3)^2 + 1)^{\frac{1}{2}}</span>, which is a convex and
differentiable function with only one local minimum.</p>
<p><br></p>
<h3 id="problem-5.1">Problem 5.1</h3>
<p>Perform by hand two iterations of the gradient descent algorithm on
this function, using an initial prediction of <span
class="math inline">h_0 = 2</span> and a learning rate of <span
class="math inline">\alpha = 2\sqrt{2}</span>. Show your work and your
final answers, <span class="math inline">h_1</span> and <span
class="math inline">h_2</span>.</p>
<p><br></p>
<h3 id="problem-5.2">Problem 5.2</h3>
<p>With more iterations, will we eventually converge to the minimizer?
Explain.</p>
<p><br></p>
<hr />
<h2 id="problem-6">Problem 6</h2>
<p>In general, the logarithm of a convex function is not convex. Give an
example of a function <span class="math inline">f(x)</span> such that
<span class="math inline">f(x)</span> is convex, but <span
class="math inline">\log_{10}(f(x))</span> is not convex.</p>
<hr />
<h2 id="problem-7">Problem 7</h2>
<!-- **Gradient Descent** Problem -->
Consider the following function <span class="math inline">f(x)</span>
defined below
<span class="math display">\begin{aligned}
f(x)=\max(0,1-x).

\end{aligned}</span>
<p><br></p>
<h3 id="problem-7.1">Problem 7.1</h3>
<p>Plot <span class="math inline">f(x)</span>.</p>
<p><br></p>
<h3 id="problem-7.2">Problem 7.2</h3>
Consider the following smoothed version of <span
class="math inline">f(x)</span>.
<span class="math display">\begin{aligned}
f_s(x)=\begin{cases}
0 &amp; \text{ if } x\geq 1\\
\frac12(1-x)^2 &amp; \text{ if } 0 &lt; x &lt; 1\\
0.5-x &amp; \text{ if } x\leq 0
\end{cases}.

\end{aligned}</span>
<p>Is the global minima of <span class="math inline">f_s(x)</span>
unique?</p>
<p><br></p>
<h3 id="problem-7.3">Problem 7.3</h3>
<p>Perform two steps of gradient descent with step size <span
class="math inline">\alpha=1</span> for <span
class="math inline">f_s(x)</span> starting from the point <span
class="math inline">x_0=-0.5</span>.</p>
<p><br></p>
<hr />
<h2 id="problem-8">Problem 8</h2>
<p>Suppose there is a dataset contains four values: <span
class="math inline">-2</span>, <span class="math inline">-1</span>,
<span class="math inline">2</span>, <span class="math inline">4</span>.
You would like to use gradient descent to minimize the mean square error
over this dataset.</p>
<p><br></p>
<h3 id="problem-8.1">Problem 8.1</h3>
<p>Write down the expression of mean square error and its derivative
given this dataset.</p>
<p><br></p>
<h3 id="problem-8.2">Problem 8.2</h3>
<p>Suppose you choose the initial position to be <span
class="math inline">h_0</span> and the learning rate to be <span
class="math inline">\frac{1}{4}</span>. After two gradient descent
steps, <span class="math inline">h_2=\frac{1}{4}</span>. What is the
value of <span class="math inline">h_0</span>?</p>
<p><br></p>
<h3 id="problem-8.3">Problem 8.3</h3>
<p>Given that we set the tolerance of gradient descent to be <span
class="math inline">0.1</span>. How many <strong>additional
steps</strong> beyond <span class="math inline">h_2</span> do we need to
take to reach convergence?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> 0</p></li>
<li><p><input type="radio" disabled="" /> 1</p></li>
<li><p><input type="radio" disabled="" /> 2</p></li>
<li><p><input type="radio" disabled="" /> 3</p></li>
<li><p><input type="radio" disabled="" /> 4</p></li>
<li><p><input type="radio" disabled="" /> It will never converge</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-9">Problem 9</h2>
<p>The hyperbolic cosine function is defined as <span
class="math inline">cosh(x) = \frac{1}{2}(e^{x} + e^{-x})</span>. In
this problem, we aim to prove the convexity of this function using power
series expansion.</p>
<p><br></p>
<h3 id="problem-9.1">Problem 9.1</h3>
<p>Prove that <span class="math inline">f(x) = x^{n}</span> is convex if
n is an even integer.</p>
<p><br></p>
<h3 id="problem-9.2">Problem 9.2</h3>
<p>Power series expansion is a powerful tool to analyze complicated
functions. In power series expansion, a function can be written as an
infinite sum of polynomial functions with certain coefficients. For
example, the exponential function can be written as: <span
class="math display">\begin{align*}
        e^{x} = \sum_{n=0}^{\infty}\frac{x^{n}}{n!} = 1 + x +
\frac{x^{2}}{2} + \frac{x^{3}}{6} + \frac{x^{4}}{24} + ...
\end{align*}</span></p>
<p>where <span class="math inline">n!</span> denotes the factorial of
<span class="math inline">n</span>, defined as the product of all
positive integers up to <span class="math inline">n</span>, i.e. <span
class="math inline">n! = 1\cdot 2\cdot 3\cdot  ... \cdot
(n-1)\cdot  n</span>. Given the power series expansion of <span
class="math inline">e^{x}</span> above, write the power series expansion
of <span class="math inline">e^{-x}</span> and explicitly specify the
first 5 terms, i.e., similar to the format of the equation above. <!-- 
Equation [\[exp_expand\]](#exp_expand){reference-type="ref"
reference="exp_expand"}: --></p>
<p><br></p>
<p><br></p>
<h3 id="problem-9.3">Problem 9.3</h3>
<p>Using the conclusions you reached in part (a) and part (b), prove
that <span class="math inline">cosh(x) = \frac{1}{2}(e^{x} +
e^{-x})</span> is convex.</p>
<p><br></p>
<hr />
<h2 id="problem-10">Problem 10</h2>
<p>Suppose that we are given <span class="math inline">f(x) = x^3 +
x^2</span> and learning rate <span class="math inline">\alpha =
1/4</span>.</p>
<p><br></p>
<h3 id="problem-10.1">Problem 10.1</h3>
<p>Write down the updating rule for gradient descent in general, then
write down the updating rule for gradient descent for the function <span
class="math inline">f(x)</span>.</p>
<p><br></p>
<h3 id="problem-10.2">Problem 10.2</h3>
<p>If we start at <span class="math inline">x_0 = -1</span>, should we
go left or right? Can you verify this mathematically? What is <span
class="math inline">x_1</span>? Can gradient descent converge? If so,
where it might converge to, given appropriate step size?</p>
<p><br></p>
<h3 id="problem-10.3">Problem 10.3</h3>
<p>If we start at <span class="math inline">x_0 = 1</span>, should we go
left or right? Can you verify this mathematically? What is <span
class="math inline">x_1</span>? Can gradient descent converge? If so,
where it might converge to, given appropriate step size?</p>
<p><br></p>
<h3 id="problem-10.4">Problem 10.4</h3>
<p>Write down <span class="math inline">1</span> condition to terminate
the gradient descent algorithm (in general).</p>
<p><br></p>
<hr />
<h2 id="section"><span class="math display"> </span></h2>
<h4
id="feedback-find-an-error-still-confused-have-a-suggestion-let-us-know-here.">👋
Feedback: Find an error? Still confused? Have a suggestion? Let us know
<a href="https://forms.gle/xK4DpWXh9rq8AKP37">here</a>.</h4>
<hr />
</body>
</html>
