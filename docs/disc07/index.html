<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Summary Statistics and Linear Regression</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../assets/theme.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Summary Statistics and Linear Regression</h1>
</header>
<p><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
<!-- add after bootstrap.min.css -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<!-- add after bootstrap.min.js or bootstrap.bundle.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script></p>
<!-- for difficulty gauges-->
<script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-B947E6J6H4"></script> -->
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  // gtag('js', new Date());

  // gtag('config', 'G-B947E6J6H4');
</script>
<p><a href="../index.html">← return to study.practicaldsc.org</a></p>
<hr />
<p>The problems in this worksheet are taken from past exams in similar
classes. Work on them <strong>on paper</strong>, since the exams you
take in this course will also be on paper. <br><br>We encourage you to
complete this worksheet in a live discussion section. Solutions will be
made available after all discussion sections have concluded. You don’t
need to submit your answers anywhere.<br><br><b>Note: We do not plan to
cover all problems here in the live discussion section</b>; the problems
we don’t cover can be used for extra practice.</p>
<hr />
<h2 id="problem-1">Problem 1</h2>
<p>Biff the Wolverine just made an Instagram account and has been
keeping track of the number of likes his posts have received so far.</p>
<p>His first 7 posts have received a mean of 16 likes; the specific like
counts in sorted order are</p>
<p><span class="math display">8, 12, 12, 15, 18, 20, 27</span></p>
<p>Biff the Wolverine wants to predict the number of likes his next post
will receive, using a constant prediction rule <span
class="math inline">h</span>. For each loss function <span
class="math inline">L(y_i, h)</span>, determine the constant prediction
<span class="math inline">h^*</span> that minimizes average loss. If you
believe there are multiple minimizers, specify them all. If you believe
you need more information to answer the question or that there is no
minimizer, state that clearly. <strong>Give a brief justification for
each answer.</strong></p>
<p><br></p>
<h3 id="problem-1.1">Problem 1.1</h3>
<p><span class="math inline">L(y_i, h) = |y_i - h|</span></p>
<p><br></p>
<h3 id="problem-1.2">Problem 1.2</h3>
<p><span class="math inline">L(y_i, h) = (y_i - h)^2</span></p>
<p><br></p>
<h3 id="problem-1.3">Problem 1.3</h3>
<p><span class="math inline">L(y_i, h) = 4(y_i - h)^2</span></p>
<p><br></p>
<h3 id="problem-1.4">Problem 1.4</h3>
<p><span class="math inline">L(y_i, h) = \begin{cases} 0 &amp; h = y_i
\\ 100 &amp; h \neq y_i \end{cases}</span></p>
<p><br></p>
<h3 id="problem-1.5">Problem 1.5</h3>
<p><span class="math inline">L(y_i, h) = (3y_i - 4h)^2</span></p>
<p><br></p>
<h3 id="problem-1.6">Problem 1.6</h3>
<p><span class="math inline">L(y_i, h) = (y_i - h)^3</span></p>
<p><em>Hint: Do not spend too long on this subpart.</em></p>
<p><br></p>
<hr />
<h2 id="problem-2">Problem 2</h2>
<p>You may find the following properties of logarithms helpful in this
question. Assume that all logarithms in this question are natural
logarithms, i.e. of base <span class="math inline">e</span>.</p>
<ul>
<li><span class="math inline">e^{\log(x)} = x</span></li>
<li><span class="math inline">\log(a) + \log(b) = \log(a \cdot
b)</span></li>
<li><span class="math inline">\log(a) - \log(b) = \log \left(
\frac{a}{b} \right)</span></li>
<li><span class="math inline">\log(a^c) = c \log (a)</span></li>
<li><span class="math inline">\frac{d}{dx} \log x =
\frac{1}{x}</span></li>
</ul>
<p>Billy is trying his hand at coming up with loss functions. He comes
up with the Billy loss, <span class="math inline">L_B(y_i, h)</span>,
defined as follows:</p>
<p><span class="math display">L_B(y_i, h) = \left[ \log \left(
\frac{y_i}{h} \right) \right]^2</span></p>
<p>Throughout this problem, assume that all <span
class="math inline">y_i</span>s are positive.</p>
<p><br></p>
<h3 id="problem-2.1">Problem 2.1</h3>
<p>Show that: <span class="math display">\frac{d}{dh} L_B(y_i, h) = -
\frac{2}{h} \log \left( \frac{y_i}{h} \right)</span></p>
<p><br></p>
<h3 id="problem-2.2">Problem 2.2</h3>
<p>Show that the constant prediction <span
class="math inline">h^*</span> that minimizes average Billy loss for the
constant model is:</p>
<p><span class="math display">h^* = \left(y_1 \cdot y_2 \cdot ... \cdot
y_n \right)^{\frac{1}{n}}</span></p>
<p>You do not need to perform a second derivative test, but otherwise
you must show your work.</p>
<p><em>Hint: To confirm that you’re interpreting the result correctly,
<span class="math inline">h^*</span> for the dataset 3, 5, 16 is <span
class="math inline">(3 \cdot 5 \cdot 16)^{\frac{1}{3}} =
240^{\frac{1}{3}} \approx 6.214</span>.</em></p>
<p><br></p>
<hr />
<h2 id="problem-3">Problem 3</h2>
<p>Billy decides to take on a part-time job as a waiter at the Panda
Express in Pierpont. For two months, he kept track of all of the total
bills he gave out to customers along with the tips they then gave him,
all in dollars. Below is a scatter plot of Billy’s tips and total
bills.</p>
<!-- TODO -->
<center><img src="../assets/images/disc07/dirtybirds.png" width="600" height="330"></center>
<p>Throughout this question, assume we are trying to fit a linear
prediction rule <span class="math inline">H(x) = w_0 + w_1x</span> that
uses total bills to predict tips, and assume we are finding optimal
parameters by minimizing mean squared error.</p>
<p><br></p>
<h3 id="problem-3.1">Problem 3.1</h3>
<p>Which of these is the most likely value for <span
class="math inline">r</span>, the correlation between total bill and
tips? Why?</p>
<div class="center">
<p><span class="math display">-1 \qquad -0.75 \qquad -0.25 \qquad 0
\qquad 0.25 \qquad 0.75 \qquad 1</span></p>
</div>
<p><br></p>
<h3 id="problem-3.2">Problem 3.2</h3>
<p>The variance of the tip amounts is 2.1. Let <span
class="math inline">M</span> be the mean squared error of the best
linear prediction rule on this dataset (under squared loss). Is <span
class="math inline">M</span> less than, equal to, or greater than 2.1?
How can you tell?</p>
<p><br></p>
<h3 id="problem-3.3">Problem 3.3</h3>
<p>Suppose we use the formulas from class on Billy’s dataset and
calculate the optimal slope <span class="math inline">w_1^*</span> and
intercept <span class="math inline">w_0^*</span> for this prediction
rule.</p>
<p>Suppose we add the value of 1 to every total bill <span
class="math inline">x</span>, effectively shifting the scatter plot 1
unit to the right. <strong>Note that doing this does not change the
value of <span class="math inline">w_1^*</span>.</strong> What amount
should we add to each tip <span class="math inline">y</span> so that the
value of <span class="math inline">w_0^*</span> also does not change?
Your answer should involve one or more of <span
class="math inline">\bar{x}, \bar{y}, w_0^*, w_1^*,</span> and any
constants.</p>
<p><em>Note: To receive full points, you must provide a rigorous
explanation, though this explanation only takes a few lines. However, we
will award partial credit to solutions with the correct answer, and it’s
possible to arrive at the correct answer by drawing a picture and
thinking intuitively about what happens.</em></p>
<p><br></p>
<hr />
<h2 id="problem-4">Problem 4</h2>
<p>Suppose we have a dataset of <span class="math inline">n</span>
houses that were recently sold in the Ann Arbor area. For each house, we
have its square footage and most recent sale price. The correlation
between square footage and price is <span
class="math inline">r</span>.</p>
<p><br></p>
<h3 id="problem-4.1">Problem 4.1</h3>
<p>First, we minimize mean squared error to fit a linear prediction rule
that uses square footage to predict price. The resulting prediction rule
has an intercept of <span class="math inline">w_0^*</span> and slope of
<span class="math inline">w_1^*</span>. In other words,</p>
<p><span class="math display">\text{predicted price} = w_0^* + w_1^*
\cdot \text{square footage}</span></p>
<p>We’re now interested in minimizing mean squared error to fit a linear
prediction rule <strong>that uses price to predict square
footage</strong>. Suppose this new regression line has an intercept of
<span class="math inline">\beta_0^*</span> and slope of <span
class="math inline">\beta_1^*</span>.</p>
<p>What is <span class="math inline">\beta_1^*</span>? Give your answer
in terms of one or more of <span class="math inline">n</span>, <span
class="math inline">r</span>, <span class="math inline">w_0^*</span>,
and <span class="math inline">w_1^*</span>. Show your work.</p>
<p><br></p>
<h3 id="problem-4.2">Problem 4.2</h3>
<p><strong>For this part only</strong>, assume that the following
quantities hold:</p>
<ul>
<li><span class="math inline">n = 100</span></li>
<li><span class="math inline">r = 0.6</span></li>
<li><span class="math inline">w_0^* = 1000</span></li>
<li><span class="math inline">w_1^* = 250</span></li>
<li>The average square footage of homes in the dataset is 2000</li>
</ul>
<p>Given this information, what is <span
class="math inline">\beta_0^*</span>? Give your answer as a constant,
rounded to two decimal places. Show your work.</p>
<p><br></p>
<hr />
<h2 id="problem-5">Problem 5</h2>
<p>The mean of 12 non-negative numbers is 45. Suppose we remove 2 of
these numbers. What is the largest possible value of the mean of the
remaining 10 numbers? Show your work.</p>
<hr />
<h2 id="problem-6">Problem 6</h2>
<!--  -->
<p>Let <span class="math inline">R_{sq}(h)</span> represent the mean
squared error of a constant prediction <span
class="math inline">h</span> for a given dataset. Find a dataset <span
class="math inline">\{y_1, y_2\}</span> such that the graph of <span
class="math inline">R_{sq}(h)</span> has its minimum at the point <span
class="math inline">(7,16)</span>.</p>
<hr />
<h2 id="problem-7">Problem 7</h2>
<!--  -->
<p>Consider a dataset <span class="math inline">D</span> with <span
class="math inline">5</span> data points <span
class="math inline">\{7,5,1,2,a\}</span>, where a is a positive real
number. Note that <span class="math inline">a</span> is not necessarily
an integer.</p>
<p><br></p>
<h3 id="problem-7.1">Problem 7.1</h3>
<p>Express the mean of <span class="math inline">D</span> as a function
of <span class="math inline">a</span>, simplify the expression as much
as possible.</p>
<p><br></p>
<h3 id="problem-7.2">Problem 7.2</h3>
<p>Depending on the range of <span class="math inline">a</span>, the
median of <span class="math inline">D</span> could assume one of three
possible values. Write out all possible median of <span
class="math inline">D</span> along with the corresponding range of <span
class="math inline">a</span> for each case. Express the ranges using
double inequalities, e.g., i.e. <span
class="math inline">3&lt;a\leq8</span>:</p>
<p><br></p>
<h3 id="problem-7.3">Problem 7.3</h3>
<p>Determine the range of <span class="math inline">a</span> that
satisfies: <span class="math display">\text{Mean}(D) &lt;
\text{Median}(D)</span> Make sure to show your work.</p>
<p><br></p>
<hr />
<h2 id="problem-8">Problem 8</h2>
<!--  -->
<p>Consider a dataset of <span class="math inline">n</span>
<strong>integers</strong>, <span class="math inline">y_1, y_2, ...,
y_n</span>, whose histogram is given below:</p>
<center><img src='../../assets/images/disc07/hist-dist.png' width=900></center>
<p><br></p>
<h3 id="problem-8.1">Problem 8.1</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes:</p>
<p><span class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n
\begin{cases} 0 &amp; y_i = h \\ 1 &amp; y_i \neq h
\end{cases}</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-8.2">Problem 8.2</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n |y_i -
h|</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-8.3">Problem 8.3</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n (y_i -
h)^2</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-8.4">Problem 8.4</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \lim_{p \rightarrow \infty}
\frac{1}{n} \sum_{i = 1}^n |y_i - h|^p</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-9">Problem 9</h2>
<p>Suppose there is a dataset containing 10000 integers:</p>
<ul>
<li>2500 of them are <span class="math inline">3</span>s</li>
<li>2500 of them are <span class="math inline">5</span>s</li>
<li>4500 of them are <span class="math inline">7</span>s</li>
<li>500 of them are <span class="math inline">9</span>s.</li>
</ul>
<p><br></p>
<h3 id="problem-9.1">Problem 9.1</h3>
<p>Calculate the median of this dataset.</p>
<p><br></p>
<h3 id="problem-9.2">Problem 9.2</h3>
<p>How does the mean of this dataset compared to its median?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> The mean is larger than the median</p></li>
<li><p><input type="radio" disabled="" /> The mean is smaller than the median</p></li>
<li><p><input type="radio" disabled="" /> The mean and the median are equal</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-10">Problem 10</h2>
<!--  -->
<p>Define the extreme mean (<span class="math inline">EM</span>) of a
dataset to be the average of its largest and smallest values. Let <span
class="math display">f(x)=-3x+4.</span> Show that for any dataset <span
class="math inline">x_1\leq x_2 \leq \dots \leq x_n</span>, <span
class="math display">EM(f(x_1), f(x_2), \dots, f(x_n)) = f(EM(x_1, x_2,
\dots, x_n)).</span></p>
<hr />
<h2 id="problem-11">Problem 11</h2>
<p>Consider a dataset of <span class="math inline">n</span> values,
<span class="math inline">y_1, y_2, ..., y_n</span>, all of which are
non-negative. We’re interested in fitting a constant model, <span
class="math inline">H(x) = h</span>, to the data, using the new
“Wolverine” loss function:</p>
<p><span class="math display">L_\text{wolverine}(y_i, h) = w_i \left(
y_i^2 - h^2  \right)^2</span></p>
<p>Here, <span class="math inline">w_i</span> corresponds to the
“weight” assigned to the data point <span
class="math inline">y_i</span>, the idea being that different data
points can be weighted differently when finding the optimal constant
prediction, <span class="math inline">h^*</span>.</p>
<p>For example, for the dataset <span class="math inline">y_1 = 1, y_2 =
5, y_3 = 2</span>, we will end up with different values of <span
class="math inline">h^*</span> when we use the weights <span
class="math inline">w_1 = w_2 = w_3 = 1</span> and when we use weights
<span class="math inline">w_1 = 8, w_2 = 4, w_3 = 3</span>.</p>
<p><br></p>
<h3 id="problem-11.1">Problem 11.1</h3>
<p>Find <span class="math inline">\frac{\partial
L_\text{wolverine}}{\partial h}</span>, the derivative of the Wolverine
loss function with respect to <span class="math inline">h</span>. Show
your work.</p>
<p><br></p>
<h3 id="problem-11.2">Problem 11.2</h3>
<p>Prove that the constant prediction that minimizes average loss for
the Wolverine loss function is:</p>
<p><span class="math display">h^* = \sqrt{\frac{\sum_{i = 1}^n w_i
y_i^2}{\sum_{i = 1}^n w_i}}</span></p>
<p><br></p>
<h3 id="problem-11.3">Problem 11.3</h3>
<p>For a dataset of non-negative values <span class="math inline">y_1,
y_2, ..., y_n</span> with weights <span class="math inline">w_1, 1, ...,
1</span>, evaluate: <span class="math display">\displaystyle \lim_{w_1
\rightarrow \infty} h^*</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> The maximum of <span class="math inline">y_1, y_2, ...,
y_n</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_1, y_2, ...,
y_{n-1}</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_2, y_3, ..., y_n</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_2, y_3, ..., y_n</span>,
multiplied by <span class="math inline">\frac{n}{n-1}</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">y_1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">y_n</span></p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-12">Problem 12</h2>
<p>Suppose we’re given a dataset of <span class="math inline">n</span>
points, <span class="math inline">(x_1, y_1), (x_2, y_2), ..., (x_n,
y_n)</span>, where <span class="math inline">\bar{x}</span> is the mean
of <span class="math inline">x_1, x_2, ..., x_n</span> and <span
class="math inline">\bar{y}</span> is the mean of <span
class="math inline">y_1, y_2, ..., y_n</span>.</p>
<p>Using this dataset, we create a <em>transformed</em> dataset of <span
class="math inline">n</span> points, <span
class="math inline">(x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ...,
(x_n&#39;, y_n&#39;)</span>, where:</p>
<p><span class="math display">x_i&#39; = 4x_i - 3 \qquad y_i&#39; = y_i
+ 24</span></p>
<p>That is, the transformed dataset is of the form <span
class="math inline">(4x_1 - 3, y_1 + 24), ..., (4x_n - 3, y_n +
24)</span>.</p>
<p>We decide to fit a simple linear hypothesis function <span
class="math inline">H(x&#39;) = w_0 + w_1x&#39;</span> on the
transformed dataset using squared loss. We find that <span
class="math inline">w_0^* = 7</span> and <span class="math inline">w_1^*
= 2</span>, so <span class="math inline">H^*(x&#39;) = 7 +
2x&#39;</span>.</p>
<p><br></p>
<h3 id="problem-12.1">Problem 12.1</h3>
<p>Suppose we were to fit a simple linear hypothesis function through
the original dataset, <span class="math inline">(x_1, y_1), (x_2, y_2),
..., (x_n, y_n)</span>, again using squared loss. What would the optimal
slope be?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">2</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">4</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">8</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">12</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">24</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-12.2">Problem 12.2</h3>
<p>Recall, the hypothesis function <span class="math inline">H^*</span>
was fit on the transformed dataset,</p>
<p><span class="math inline">(x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;),
..., (x_n&#39;, y_n&#39;)</span>. <span class="math inline">H^*</span>
happens to pass through the point <span class="math inline">(\bar{x},
\bar{y})</span>. What is the value of <span
class="math inline">\bar{x}</span>? Give your answer as an integer with
no variables.</p>
<p><br></p>
<hr />
<h2 id="problem-13">Problem 13</h2>
<p>For a given dataset <span class="math inline">\{y_1, y_2, \dots,
y_n\}</span>, let <span class="math inline">M_{abs}(h)</span> represent
the <strong>median</strong> absolute error of the constant prediction
<span class="math inline">h</span> on that dataset (as opposed to the
mean absolute error <span class="math inline">R_{abs}(h)</span>).</p>
<p><br></p>
<h3 id="problem-13.1">Problem 13.1</h3>
<p>For the dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, what is <span class="math inline">M_{abs}(9)</span>?</p>
<p><br></p>
<h3 id="problem-13.2">Problem 13.2</h3>
<p>For the same dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, find another integer <span class="math inline">h</span>
such that <span class="math inline">M_{abs}(9) = M_{abs}(h)</span>.</p>
<p><br></p>
<h3 id="problem-13.3">Problem 13.3</h3>
<p>Based on your answers to parts (a) and (b), discuss in <strong>at
most two sentences</strong> what is problematic about using the median
absolute error to make predictions.</p>
<p><br></p>
<hr />
<h2 id="problem-14">Problem 14</h2>
<p>Suppose we are given a dataset of points <span
class="math inline">\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}</span>
and for some reason, we want to make predictions using a prediction rule
of the form <span class="math display">H(x) = 17 + w_1x.</span></p>
<p><br></p>
<h3 id="problem-14.1">Problem 14.1</h3>
<p>Write down an expression for the mean squared error of a prediction
rule of this form, as a function of the parameter <span
class="math inline">w_1</span>.</p>
<p><br></p>
<h3 id="problem-14.2">Problem 14.2</h3>
<p>Minimize the function <span class="math inline">MSE(w_1)</span> to
find the parameter <span class="math inline">w_1^*</span> which defines
the optimal prediction rule <span class="math inline">H^*(x) = 17 +
w_1^*x</span>. Show all your work and explain your steps.</p>
<p>Fill in your final answer below:</p>
<p><br></p>
<h3 id="problem-14.3">Problem 14.3</h3>
<p>True or False: For an arbitrary dataset, the prediction rule <span
class="math inline">H^*(x) = 17 + w_1^*x</span> goes through the point
<span class="math inline">(\bar x, \bar y)</span>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<h3 id="problem-14.4">Problem 14.4</h3>
<p>True or False: For an arbitrary dataset, the mean squared error
associated with <span class="math inline">H^*(x)</span> is greater than
or equal to the mean squared error associated with the regression
line.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-15">Problem 15</h2>
<p>Suppose you have a dataset <span class="math display">\{(x_1, y_1),
(x_2,y_2), \dots, (x_8, y_8)\}</span> with <span
class="math inline">n=8</span> ordered pairs such that the variance of
<span class="math inline">\{x_1, x_2, \dots, x_8\}</span> is <span
class="math inline">50</span>. Let <span class="math inline">m</span> be
the slope of the regression line fit to this data.</p>
<p>Suppose now we fit a regression line to the dataset <span
class="math display">\{(x_1, y_2), (x_2,y_1), \dots, (x_8, y_8)\}</span>
where the first two <span class="math inline">y</span>-values have been
swapped. Let <span class="math inline">m&#39;</span> be the slope of
this new regression line.</p>
<p>If <span class="math inline">x_1 = 3</span>, <span
class="math inline">y_1 =7</span>, <span
class="math inline">x_2=8</span>, and <span
class="math inline">y_2=2</span>, what is the difference between the new
slope and the old slope? That is, what is <span
class="math inline">m&#39; - m</span>? The answer you get should be a
number with no variables.</p>
<p><strong>Hint:</strong> There are many equivalent formulas for the
slope of the regression line. We recommend using the version of the
formula without <span class="math inline">\overline{y}</span>.</p>
<hr />
<h2 id="problem-16">Problem 16</h2>
<p>Note that we have two simplified closed form expressions for the
estimated slope <span class="math inline">w</span> in simple linear
regression that you have already seen in discussions and lectures:</p>
<p><span class="math display">
\begin{align*}
    w &amp;= \frac{\sum_i (x_i - \overline{x}) y_i}{\sum_i (x_i -
\overline{x})^2} \\ \\
    w &amp;= \frac{\sum_i (y_i - \overline{y}) x_i }{\sum_i (x_i -
\overline{x})^2}
\end{align*}
</span></p>
<p>where we have dataset <span class="math inline">D = [(x_1,y_1),
\ldots, (x_n,y_n)]</span> and sample means   <span
class="math inline">\overline{x} = {1 \over n} \sum_{i} x_i, \quad
\overline{y} = {1 \over n} \sum_{i} y_i</span>. Without further
explanation, <span class="math inline">\sum_i</span> means <span
class="math inline">\sum_{i=1}^n</span></p>
<p><br></p>
<h3 id="problem-16.1">Problem 16.1</h3>
<p>Are (<span class="math inline">1</span>) and (<span
class="math inline">2</span>) equivalent? That is, is the following
equality true? Prove or disprove it. <span class="math display">\sum_i
(x_i - \overline{x}) y_i = \sum_i (y_i - \overline{y}) x_i</span></p>
<p><br></p>
<h3 id="problem-16.2">Problem 16.2</h3>
<p>True or False: If the dataset shifted right by a constant distance
<span class="math inline">a</span>, that is, we have the new dataset
<span class="math inline">D_a = (x_1 + a,y_1), \ldots, (x_n +
a,y_n)</span>, then will the estimated slope <span
class="math inline">w</span> change or not?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<h3 id="problem-16.3">Problem 16.3</h3>
<p>True or False: If the dataset shifted up by a constant distance <span
class="math inline">b</span>, that is, we have the new dataset <span
class="math inline">D_b = [(x_1,y_1 + b), \ldots, (x_n,y_n + b)]</span>,
then will the estimated slope <span class="math inline">w</span> change
or not?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-17">Problem 17</h2>
<p>Consider a dataset that consists of <span class="math inline">y_1,
\cdots, y_n</span>. In class, we used calculus to minimize mean squared
error, <span class="math inline">R_{sq}(h) = \frac{1}{n} \sum_{i = 1}^n
(h - y_i)^2</span>. In this problem, we want you to apply the same
approach to a slightly different loss function defined below: <span
class="math display">L_{\text{midterm}}(y,h)=(\alpha y - h)^2+\lambda
h</span></p>
<p><br></p>
<h3 id="problem-17.1">Problem 17.1</h3>
<p>Write down the empiricial risk <span
class="math inline">R_{\text{midterm}}(h)</span> by using the above loss
function.</p>
<p><br></p>
<h3 id="problem-17.2">Problem 17.2</h3>
<p>The mean of dataset is <span class="math inline">\bar{y}</span>, i.e.
<span class="math inline">\bar{y} = \frac{1}{n} \sum_{i = 1}^n
y_i</span>. Find <span class="math inline">h^*</span> that minimizes
<span class="math inline">R_{\text{midterm}}(h)</span> using calculus.
Your result should be in terms of <span
class="math inline">\bar{y}</span>, <span
class="math inline">\alpha</span> and <span
class="math inline">\lambda</span>.</p>
<p><br></p>
<hr />
<h2 id="problem-18">Problem 18</h2>
<p>For a given dataset <span class="math inline">\{y_1, y_2, \dots,
y_n\}</span>, let <span class="math inline">M_{abs}(h)</span> represent
the <strong>median</strong> absolute error of the constant prediction
<span class="math inline">h</span> on that dataset (as opposed to the
mean absolute error <span class="math inline">R_{abs}(h)</span>).</p>
<p><br></p>
<h3 id="problem-18.1">Problem 18.1</h3>
<p>For the dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, what is <span class="math inline">M_{abs}(9)</span>?</p>
<p><br></p>
<h3 id="problem-18.2">Problem 18.2</h3>
<p>For the same dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, find another integer <span class="math inline">h</span>
such that <span class="math inline">M_{abs}(9) = M_{abs}(h)</span>.</p>
<p><br></p>
<h3 id="problem-18.3">Problem 18.3</h3>
<p>Based on your answers to parts (a) and (b), discuss in <strong>at
most two sentences</strong> what is problematic about using the median
absolute error to make predictions.</p>
<p><br></p>
<hr />
<h2 id="section"><span class="math display"> </span></h2>
<h4
id="feedback-find-an-error-still-confused-have-a-suggestion-let-us-know-here.">👋
Feedback: Find an error? Still confused? Have a suggestion? Let us know
<a href="https://forms.gle/xK4DpWXh9rq8AKP37">here</a>.</h4>
<hr />
</body>
</html>
