<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Summary Statistics and the Constant Model</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../assets/theme.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Summary Statistics and the Constant Model</h1>
</header>
<p><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
<!-- add after bootstrap.min.css -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<!-- add after bootstrap.min.js or bootstrap.bundle.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script></p>
<!-- for difficulty gauges-->
<script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-B947E6J6H4"></script> -->
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  // gtag('js', new Date());

  // gtag('config', 'G-B947E6J6H4');
</script>
<p><a href="../index.html">← return to study.practicaldsc.org</a></p>
<hr />
<p>The problems in this worksheet are taken from past exams in similar
classes. Work on them <strong>on paper</strong>, since the exams you
take in this course will also be on paper. <br><br>We encourage you to
complete this worksheet in a live discussion section. Solutions will be
made available after all discussion sections have concluded. You don’t
need to submit your answers anywhere.<br><br><b>Note: We do not plan to
cover all problems here in the live discussion section</b>; the problems
we don’t cover can be used for extra practice.</p>
<hr />
<h2 id="problem-1">Problem 1</h2>
<!-- <i>Source: [Fall 2021 Final Exam](../fa21-final/index.html), Problem 1</i> -->
<p>The mean of 12 non-negative numbers is 45. Suppose we remove 2 of
these numbers. What is the largest possible value of the mean of the
remaining 10 numbers? Show your work.</p>
<hr />
<h2 id="problem-2">Problem 2</h2>
<!-- <i>Source: [Fall 2021 Final Exam](../fa21-final/index.html), Problem 4</i> -->
<p>You may find the following properties of logarithms helpful in this
question. Assume that all logarithms in this question are natural
logarithms, i.e. of base <span class="math inline">e</span>.</p>
<ul>
<li><span class="math inline">e^{\log(x)} = x</span></li>
<li><span class="math inline">\log(a) + \log(b) = \log(a \cdot
b)</span></li>
<li><span class="math inline">\log(a) - \log(b) = \log \left(
\frac{a}{b} \right)</span></li>
<li><span class="math inline">\log(a^c) = c \log (a)</span></li>
<li><span class="math inline">\frac{d}{dx} \log x =
\frac{1}{x}</span></li>
</ul>
<p>Billy, the avocado-farmer-turned-waiter-turned-Instagram-influencer
that you’re all-too-familiar with, is trying his hand at coming up with
loss functions. He comes up with the Billy loss, <span
class="math inline">L_B(y_i, h)</span>, defined as follows:</p>
<p><span class="math display">L_B(y_i, h) = \left[ \log \left(
\frac{y_i}{h} \right) \right]^2</span></p>
<p>Throughout this problem, assume that all <span
class="math inline">y</span>s are positive.</p>
<p><br></p>
<h3 id="problem-2.1">Problem 2.1</h3>
<p>Show that: <span class="math display">\frac{d}{dh} L_B(y_i, h) = -
\frac{2}{h} \log \left( \frac{y_i}{h} \right)</span></p>
<p><br></p>
<h3 id="problem-2.2">Problem 2.2</h3>
<p>Show that the constant prediction <span
class="math inline">h^*</span> that minimizes average loss for Billy
loss is:</p>
<p><span class="math display">h^* = \left(y_1 \cdot y_2 \cdot ... \cdot
y_n \right)^{\frac{1}{n}}</span></p>
<p>You do not need to perform a second derivative test, but otherwise
you must show your work.</p>
<p><em>Hint: To confirm that you’re interpreting the result correctly,
<span class="math inline">h^*</span> for the dataset 3, 5, 16 is <span
class="math inline">(3 \cdot 5 \cdot 16)^{\frac{1}{3}} =
240^{\frac{1}{3}} \approx 6.214</span>.</em></p>
<p><br></p>
<hr />
<h2 id="problem-3">Problem 3</h2>
<!-- <i>Source: [Fall 2021 Midterm](../fa21-midterm/index.html), Problem 1</i> -->
<p>Biff the Wolverine just made an Instagram account and has been
keeping track of the number of likes his posts have received so far.</p>
<p>His first 7 posts have received a mean of 16 likes; the specific like
counts in sorted order are</p>
<p><span class="math display">8, 12, 12, 15, 18, 20, 27</span></p>
<p>Biff the Wolverine wants to predict the number of likes his next post
will receive, using a constant prediction rule <span
class="math inline">h</span>. For each loss function <span
class="math inline">L(y_i, h)</span>, determine the constant prediction
<span class="math inline">h^*</span> that minimizes average loss. If you
believe there are multiple minimizers, specify them all. If you believe
you need more information to answer the question or that there is no
minimizer, state that clearly. <strong>Give a brief justification for
each answer.</strong></p>
<p><br></p>
<h3 id="problem-3.1">Problem 3.1</h3>
<p><span class="math inline">L(y_i, h) = |y_i - h|</span></p>
<p><br></p>
<h3 id="problem-3.2">Problem 3.2</h3>
<p><span class="math inline">L(y_i, h) = (y_i - h)^2</span></p>
<p><br></p>
<h3 id="problem-3.3">Problem 3.3</h3>
<p><span class="math inline">L(y_i, h) = 4(y_i - h)^2</span></p>
<p><br></p>
<h3 id="problem-3.4">Problem 3.4</h3>
<p><span class="math inline">L(y_i, h) = \begin{cases} 0 &amp; h = y_i
\\ 100 &amp; h \neq y_i \end{cases}</span></p>
<p><br></p>
<h3 id="problem-3.5">Problem 3.5</h3>
<p><span class="math inline">L(y_i, h) = (3y_i - 4h)^2</span></p>
<p><br></p>
<h3 id="problem-3.6">Problem 3.6</h3>
<p><span class="math inline">L(y_i, h) = (y_i - h)^3</span></p>
<p><em>Hint: Do not spend too long on this subpart.</em></p>
<p><br></p>
<hr />
<h2 id="problem-4">Problem 4</h2>
<!-- <i>Source: [Spring 2023 Midterm 1](../sp23-midterm1/index.html), Problem 2</i> -->
<p>Let <span class="math inline">R_{sq}(h)</span> represent the mean
squared error of a constant prediction <span
class="math inline">h</span> for a given dataset. Find a dataset <span
class="math inline">\{y_1, y_2\}</span> such that the graph of <span
class="math inline">R_{sq}(h)</span> has its minimum at the point <span
class="math inline">(7,16)</span>.</p>
<hr />
<h2 id="problem-5">Problem 5</h2>
<!-- <i>Source: [Winter 2024 Midterm 1](../wi24-midterm1/index.html), Problem 1</i> -->
<p>Consider a dataset <span class="math inline">D</span> with <span
class="math inline">5</span> data points <span
class="math inline">\{7,5,1,2,a\}</span>, where a is a positive real
number. Note that <span class="math inline">a</span> is not necessarily
an integer.</p>
<p><br></p>
<h3 id="problem-5.1">Problem 5.1</h3>
<p>Express the mean of <span class="math inline">D</span> as a function
of <span class="math inline">a</span>, simplify the expression as much
as possible.</p>
<p><br></p>
<h3 id="problem-5.2">Problem 5.2</h3>
<p>Depending on the range of <span class="math inline">a</span>, the
median of <span class="math inline">D</span> could assume one of three
possible values. Write out all possible median of <span
class="math inline">D</span> along with the corresponding range of <span
class="math inline">a</span> for each case. Express the ranges using
double inequalities, e.g., i.e. <span
class="math inline">3&lt;a\leq8</span>:</p>
<p><br></p>
<h3 id="problem-5.3">Problem 5.3</h3>
<p>Determine the range of <span class="math inline">a</span> that
satisfies: <span class="math display">\text{Mean}(D) &lt;
\text{Median}(D)</span> Make sure to show your work.</p>
<p><br></p>
<hr />
<h2 id="problem-6">Problem 6</h2>
<!-- <i>Source: [Spring 2024 Final](../sp24-final/index.html), Problem 1</i> -->
<p>Consider a dataset of <span class="math inline">n</span>
<strong>integers</strong>, <span class="math inline">y_1, y_2, ...,
y_n</span>, whose histogram is given below:</p>
<center><img src='../../assets/images/disc07/hist-dist.png' width=900></center>
<p><br></p>
<h3 id="problem-6.1">Problem 6.1</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes:</p>
<p><span class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n
\begin{cases} 0 &amp; y_i = h \\ 1 &amp; y_i \neq h
\end{cases}</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-6.2">Problem 6.2</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n |y_i -
h|</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-6.3">Problem 6.3</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \frac{1}{n} \sum_{i = 1}^n (y_i -
h)^2</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-6.4">Problem 6.4</h3>
<p>Which of the following is closest to the constant prediction <span
class="math inline">h^*</span> that minimizes: <span
class="math display">\displaystyle \lim_{p \rightarrow \infty}
\frac{1}{n} \sum_{i = 1}^n |y_i - h|^p</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">5</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">7</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">15</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">30</span></p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-7">Problem 7</h2>
<!-- <i>Source: [Winter 2024 Final Part 1](../wi24-final-pt1/index.html), Problem 1</i> -->
<p>Suppose there is a dataset containing 10000 integers:</p>
<ul>
<li>2500 of them are <span class="math inline">3</span>s</li>
<li>2500 of them are <span class="math inline">5</span>s</li>
<li>4500 of them are <span class="math inline">7</span>s</li>
<li>500 of them are <span class="math inline">9</span>s.</li>
</ul>
<p><br></p>
<h3 id="problem-7.1">Problem 7.1</h3>
<p>Calculate the median of this dataset.</p>
<p><br></p>
<h3 id="problem-7.2">Problem 7.2</h3>
<p>How does the mean of this dataset compared to its median?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> The mean is larger than the median</p></li>
<li><p><input type="radio" disabled="" /> The mean is smaller than the median</p></li>
<li><p><input type="radio" disabled="" /> The mean and the median are equal</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-8">Problem 8</h2>
<!-- <i>Source: [Winter 2022 Midterm 1](../wi22-midterm1/index.html), Problem 1</i> -->
<p>Define the extreme mean (<span class="math inline">EM</span>) of a
dataset to be the average of its largest and smallest values. Let <span
class="math display">f(x)=-3x+4.</span> Show that for any dataset <span
class="math inline">x_1\leq x_2 \leq \dots \leq x_n</span>, <span
class="math display">EM(f(x_1), f(x_2), \dots, f(x_n)) = f(EM(x_1, x_2,
\dots, x_n)).</span></p>
<hr />
<h2 id="problem-9">Problem 9</h2>
<!-- <i>Source: [Spring 2024 Midterm](../sp24-midterm/index.html), Problem 2</i> -->
<p>Consider a dataset of <span class="math inline">n</span> values,
<span class="math inline">y_1, y_2, ..., y_n</span>, all of which are
non-negative. We’re interested in fitting a constant model, <span
class="math inline">H(x) = h</span>, to the data, using the new
“Wolverine” loss function:</p>
<p><span class="math display">L_\text{wolverine}(y_i, h) = w_i \left(
y_i^2 - h^2  \right)^2</span></p>
<p>Here, <span class="math inline">w_i</span> corresponds to the
“weight” assigned to the data point <span
class="math inline">y_i</span>, the idea being that different data
points can be weighted differently when finding the optimal constant
prediction, <span class="math inline">h^*</span>.</p>
<p>For example, for the dataset <span class="math inline">y_1 = 1, y_2 =
5, y_3 = 2</span>, we will end up with different values of <span
class="math inline">h^*</span> when we use the weights <span
class="math inline">w_1 = w_2 = w_3 = 1</span> and when we use weights
<span class="math inline">w_1 = 8, w_2 = 4, w_3 = 3</span>.</p>
<p><br></p>
<h3 id="problem-9.1">Problem 9.1</h3>
<p>Find <span class="math inline">\frac{\partial
L_\text{wolverine}}{\partial h}</span>, the derivative of the Wolverine
loss function with respect to <span class="math inline">h</span>. Show
your work, and put a <span class="math inline">\boxed{\text{box}}</span>
around your final answer.</p>
<p><br></p>
<h3 id="problem-9.2">Problem 9.2</h3>
<p>Prove that the constant prediction that minimizes average loss for
the Wolverine loss function is:</p>
<p><span class="math display">h^* = \sqrt{\frac{\sum_{i = 1}^n w_i
y_i^2}{\sum_{i = 1}^n w_i}}</span></p>
<p><br></p>
<h3 id="problem-9.3">Problem 9.3</h3>
<p>For a dataset of non-negative values <span class="math inline">y_1,
y_2, ..., y_n</span> with weights <span class="math inline">w_1, 1, ...,
1</span>, evaluate: <span class="math display">\displaystyle \lim_{w_1
\rightarrow \infty} h^*</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> The maximum of <span class="math inline">y_1, y_2, ...,
y_n</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_1, y_2, ...,
y_{n-1}</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_2, y_3, ..., y_n</span></p></li>
<li><p><input type="radio" disabled="" /> The mean of <span class="math inline">y_2, y_3, ..., y_n</span>,
multiplied by <span class="math inline">\frac{n}{n-1}</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">y_1</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">y_n</span></p></li>
</ul>
<p><br></p>
<hr />
<h2 id="section"><span class="math display"> </span></h2>
<h4
id="feedback-find-an-error-still-confused-have-a-suggestion-let-us-know-here.">👋
Feedback: Find an error? Still confused? Have a suggestion? Let us know
<a href="https://forms.gle/xK4DpWXh9rq8AKP37">here</a>.</h4>
<hr />
</body>
</html>
