<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Linear Regression</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../assets/theme.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Linear Regression</h1>
</header>
<p><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
<!-- add after bootstrap.min.css -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"/>
<!-- add after bootstrap.min.js or bootstrap.bundle.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script></p>
<!-- for difficulty gauges-->
<script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-B947E6J6H4"></script> -->
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  // gtag('js', new Date());

  // gtag('config', 'G-B947E6J6H4');
</script>
<p><a href="../index.html">← return to study.practicaldsc.org</a></p>
<hr />
<p>The problems in this worksheet are taken from past exams in similar
classes. Work on them <strong>on paper</strong>, since the exams you
take in this course will also be on paper. <br><br>We encourage you to
complete this worksheet in a live discussion section. Solutions will be
made available after all discussion sections have concluded. You don’t
need to submit your answers anywhere.<br><br><b>Note: We do not plan to
cover all problems here in the live discussion section</b>; the problems
we don’t cover can be used for extra practice.</p>
<hr />
<h2 id="problem-1">Problem 1</h2>
<p>Billy decides to take on a part-time job as a waiter at the Panda
Express in Pierpont. For two months, he kept track of all of the total
bills he gave out to customers along with the tips they then gave him,
all in dollars. Below is a scatter plot of Billy’s tips and total
bills.</p>
<!-- TODO -->
<center><img src="../assets/images/disc08/dirtybirds.png" width="600" height="330"></center>
<p>Throughout this question, assume we are trying to fit a linear
prediction rule <span class="math inline">H(x) = w_0 + w_1x</span> that
uses total bills to predict tips, and assume we are finding optimal
parameters by minimizing mean squared error.</p>
<p><br></p>
<h3 id="problem-1.1">Problem 1.1</h3>
<p>Which of these is the most likely value for <span
class="math inline">r</span>, the correlation between total bill and
tips? Why?</p>
<div class="center">
<p><span class="math display">-1 \qquad -0.75 \qquad -0.25 \qquad 0
\qquad 0.25 \qquad 0.75 \qquad 1</span></p>
</div>
<p><br></p>
<h3 id="problem-1.2">Problem 1.2</h3>
<p>The variance of the tip amounts is 2.1. Let <span
class="math inline">M</span> be the mean squared error of the best
linear prediction rule on this dataset (under squared loss). Is <span
class="math inline">M</span> less than, equal to, or greater than 2.1?
How can you tell?</p>
<p><br></p>
<h3 id="problem-1.3">Problem 1.3</h3>
<p>Suppose we use the formulas from class on Billy’s dataset and
calculate the optimal slope <span class="math inline">w_1^*</span> and
intercept <span class="math inline">w_0^*</span> for this prediction
rule.</p>
<p>Suppose we add the value of 1 to every total bill <span
class="math inline">x</span>, effectively shifting the scatter plot 1
unit to the right. <strong>Note that doing this does not change the
value of <span class="math inline">w_1^*</span>.</strong> What amount
should we add to each tip <span class="math inline">y</span> so that the
value of <span class="math inline">w_0^*</span> also does not change?
Your answer should involve one or more of <span
class="math inline">\bar{x}, \bar{y}, w_0^*, w_1^*,</span> and any
constants.</p>
<p><em>Note: To receive full points, you must provide a rigorous
explanation, though this explanation only takes a few lines. However, we
will award partial credit to solutions with the correct answer, and it’s
possible to arrive at the correct answer by drawing a picture and
thinking intuitively about what happens.</em></p>
<p><br></p>
<hr />
<h2 id="problem-2">Problem 2</h2>
<p>Suppose we have a dataset of <span class="math inline">n</span>
houses that were recently sold in the Ann Arbor area. For each house, we
have its square footage and most recent sale price. The correlation
between square footage and price is <span
class="math inline">r</span>.</p>
<p><br></p>
<h3 id="problem-2.1">Problem 2.1</h3>
<p>First, we minimize mean squared error to fit a linear prediction rule
that uses square footage to predict price. The resulting prediction rule
has an intercept of <span class="math inline">w_0^*</span> and slope of
<span class="math inline">w_1^*</span>. In other words,</p>
<p><span class="math display">\text{predicted price} = w_0^* + w_1^*
\cdot \text{square footage}</span></p>
<p>We’re now interested in minimizing mean squared error to fit a linear
prediction rule <strong>that uses price to predict square
footage</strong>. Suppose this new regression line has an intercept of
<span class="math inline">\beta_0^*</span> and slope of <span
class="math inline">\beta_1^*</span>.</p>
<p>What is <span class="math inline">\beta_1^*</span>? Give your answer
in terms of one or more of <span class="math inline">n</span>, <span
class="math inline">r</span>, <span class="math inline">w_0^*</span>,
and <span class="math inline">w_1^*</span>. Show your work.</p>
<p><br></p>
<h3 id="problem-2.2">Problem 2.2</h3>
<p><strong>For this part only</strong>, assume that the following
quantities hold:</p>
<ul>
<li><span class="math inline">n = 100</span></li>
<li><span class="math inline">r = 0.6</span></li>
<li><span class="math inline">w_0^* = 1000</span></li>
<li><span class="math inline">w_1^* = 250</span></li>
<li>The average square footage of homes in the dataset is 2000</li>
</ul>
<p>Given this information, what is <span
class="math inline">\beta_0^*</span>? Give your answer as a constant,
rounded to two decimal places. Show your work.</p>
<p><br></p>
<hr />
<h2 id="problem-3">Problem 3</h2>
<p>Suppose we want to fit a hypothesis function of the form:</p>
<p><span class="math display">H(x) = w_0 + w_1 x^2</span></p>
<p>Note that this is <em>not</em> the simple linear regression
hypothesis function, <span class="math inline">H(x) = w_0 +
w_1x</span>.</p>
<p>To do so, we will find the optimal parameter vector <span
class="math inline">\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^*
\end{bmatrix}</span> that satisfies the normal equations. The first 5
rows of our dataset are as follows, though note that our dataset has
<span class="math inline">n</span> rows in total.</p>
<!-- | x  | y |
|----|---|
| 2  | 4 |
| -1 | 4 |
| 3  | 4 |
| -7 | 4 |
| 3  | 4 | -->
<table style="border: 1px solid black; border-collapse: collapse; margin: auto; text-align: center;">
  <tr>
    <th style="border: 1px solid black; padding: 8px;">x</th>
    <th style="border: 1px solid black; padding: 8px;">y</th>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">2</td>
    <td style="border: 1px solid black; padding: 8px;">4</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">-1</td>
    <td style="border: 1px solid black; padding: 8px;">4</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">3</td>
    <td style="border: 1px solid black; padding: 8px;">4</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">-7</td>
    <td style="border: 1px solid black; padding: 8px;">4</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 8px;">3</td>
    <td style="border: 1px solid black; padding: 8px;">4</td>
  </tr>
</table>
<p>Suppose that <span class="math inline">x_1, x_2, ..., x_n</span> have
a mean of <span class="math inline">\bar{x} = 2</span> and a variance of
<span class="math inline">\sigma_x^2 = 10</span>.</p>
<p><br></p>
<h3 id="problem-3.1">Problem 3.1</h3>
<p>Write out the first 5 rows of the design matrix, <span
class="math inline">X</span>.</p>
<p><br></p>
<h3 id="problem-3.2">Problem 3.2</h3>
<p>Suppose, just in part (b), that after solving the normal equations,
we find <span class="math inline">\vec{w}^* = \begin{bmatrix} 2 \\ -5
\end{bmatrix}</span>. What is the predicted <span
class="math inline">y</span> value for <span class="math inline">x =
4</span>? Give your answer as an integer with no variables. Show your
work, and put</p>
<p>a <span class="math inline">\boxed{\text{box}}</span> around your
final answer.</p>
<p><br></p>
<h3 id="problem-3.3">Problem 3.3</h3>
<p>Let <span class="math inline">X_\text{tri} = 3 X</span>. Using the
fact that <span class="math inline">\sum_{i = 1}^n x_i^2 = n \sigma_x^2
+ n \bar{x}^2</span>, determine the value of the bottom-left value in
the matrix <span class="math inline">X_\text{tri}^T X_\text{tri}</span>,
i.e. the value in the second row and first column. Give your answer as
an expression involving <span class="math inline">n</span>. Show your
work, and put a <span class="math inline">\boxed{\text{box}}</span>
around your final answer.</p>
<p><br></p>
<hr />
<h2 id="problem-4">Problem 4</h2>
<p>Suppose we’re given a dataset of <span class="math inline">n</span>
points, <span class="math inline">(x_1, y_1), (x_2, y_2), ..., (x_n,
y_n)</span>, where <span class="math inline">\bar{x}</span> is the mean
of <span class="math inline">x_1, x_2, ..., x_n</span> and <span
class="math inline">\bar{y}</span> is the mean of <span
class="math inline">y_1, y_2, ..., y_n</span>.</p>
<p>Using this dataset, we create a <em>transformed</em> dataset of <span
class="math inline">n</span> points, <span
class="math inline">(x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;), ...,
(x_n&#39;, y_n&#39;)</span>, where:</p>
<p><span class="math display">x_i&#39; = 4x_i - 3 \qquad y_i&#39; = y_i
+ 24</span></p>
<p>That is, the transformed dataset is of the form <span
class="math inline">(4x_1 - 3, y_1 + 24), ..., (4x_n - 3, y_n +
24)</span>.</p>
<p>We decide to fit a simple linear hypothesis function <span
class="math inline">H(x&#39;) = w_0 + w_1x&#39;</span> on the
transformed dataset using squared loss. We find that <span
class="math inline">w_0^* = 7</span> and <span class="math inline">w_1^*
= 2</span>, so <span class="math inline">H^*(x&#39;) = 7 +
2x&#39;</span>.</p>
<p><br></p>
<h3 id="problem-4.1">Problem 4.1</h3>
<p>Suppose we were to fit a simple linear hypothesis function through
the original dataset, <span class="math inline">(x_1, y_1), (x_2, y_2),
..., (x_n, y_n)</span>, again using squared loss. What would the optimal
slope be?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">2</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">4</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">8</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">11</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">12</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">24</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-4.2">Problem 4.2</h3>
<p>Recall, the hypothesis function <span class="math inline">H^*</span>
was fit on the transformed dataset,</p>
<p><span class="math inline">(x_1&#39;, y_1&#39;), (x_2&#39;, y_2&#39;),
..., (x_n&#39;, y_n&#39;)</span>. <span class="math inline">H^*</span>
happens to pass through the point <span class="math inline">(\bar{x},
\bar{y})</span>. What is the value of <span
class="math inline">\bar{x}</span>? Give your answer as an integer with
no variables.</p>
<p><br></p>
<hr />
<h2 id="problem-5">Problem 5</h2>
<p>Suppose you have a dataset <span class="math display">\{(x_1, y_1),
(x_2,y_2), \dots, (x_8, y_8)\}</span> with <span
class="math inline">n=8</span> ordered pairs such that the variance of
<span class="math inline">\{x_1, x_2, \dots, x_8\}</span> is <span
class="math inline">50</span>. Let <span class="math inline">m</span> be
the slope of the regression line fit to this data.</p>
<p>Suppose now we fit a regression line to the dataset <span
class="math display">\{(x_1, y_2), (x_2,y_1), \dots, (x_8, y_8)\}</span>
where the first two <span class="math inline">y</span>-values have been
swapped. Let <span class="math inline">m&#39;</span> be the slope of
this new regression line.</p>
<p>If <span class="math inline">x_1 = 3</span>, <span
class="math inline">y_1 =7</span>, <span
class="math inline">x_2=8</span>, and <span
class="math inline">y_2=2</span>, what is the difference between the new
slope and the old slope? That is, what is <span
class="math inline">m&#39; - m</span>? The answer you get should be a
number with no variables.</p>
<p><strong>Hint:</strong> There are many equivalent formulas for the
slope of the regression line. We recommend using the version of the
formula without <span class="math inline">\overline{y}</span>.</p>
<hr />
<h2 id="problem-6">Problem 6</h2>
<p>Suppose we are given a dataset of points <span
class="math inline">\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}</span>
and for some reason, we want to make predictions using a prediction rule
of the form <span class="math display">H(x) = 17 + w_1x.</span></p>
<p><br></p>
<h3 id="problem-6.1">Problem 6.1</h3>
<p>Write down an expression for the mean squared error of a prediction
rule of this form, as a function of the parameter <span
class="math inline">w_1</span>.</p>
<p><br></p>
<h3 id="problem-6.2">Problem 6.2</h3>
<p>Minimize the function <span class="math inline">MSE(w_1)</span> to
find the parameter <span class="math inline">w_1^*</span> which defines
the optimal prediction rule <span class="math inline">H^*(x) = 17 +
w_1^*x</span>. Show all your work and explain your steps.</p>
<p>Fill in your final answer below:</p>
<p><br></p>
<h3 id="problem-6.3">Problem 6.3</h3>
<p>True or False: For an arbitrary dataset, the prediction rule <span
class="math inline">H^*(x) = 17 + w_1^*x</span> goes through the point
<span class="math inline">(\bar x, \bar y)</span>.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<h3 id="problem-6.4">Problem 6.4</h3>
<p>True or False: For an arbitrary dataset, the mean squared error
associated with <span class="math inline">H^*(x)</span> is greater than
or equal to the mean squared error associated with the regression
line.</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-7">Problem 7</h2>
<p>Albert collected 400 data points from a radiation detector. Each data
point contains 3 features: feature <span class="math inline">A</span>,
feature <span class="math inline">B</span> and feature <span
class="math inline">C</span>. The true particle energy <span
class="math inline">E</span> is also reported. Albert wants to design a
linear regression algorithm to predict the energy <span
class="math inline">E</span> of each particle, given a combination of
one or more of feature <span class="math inline">A</span>, <span
class="math inline">B</span>, and <span class="math inline">C</span>. As
the first step, Albert calculated the correlation coefficients among
<span class="math inline">A</span>, <span class="math inline">B</span>,
<span class="math inline">C</span> and <span
class="math inline">E</span>. He wrote it down in the following table,
where each cell of the table represents the correlaton of two terms:</p>
<div>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><span class="math inline">A</span>   </th>
<th><span class="math inline">B</span>   </th>
<th><span class="math inline">C</span>   </th>
<th><span class="math inline">E</span>   </th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline">A</span></td>
<td>1</td>
<td>-0.99</td>
<td>0.13</td>
<td>0.8</td>
</tr>
<tr>
<td><span class="math inline">B</span></td>
<td>-0.99</td>
<td>1</td>
<td>0.25</td>
<td>-0.95</td>
</tr>
<tr>
<td><span class="math inline">C</span></td>
<td>0.13</td>
<td>0.25</td>
<td>1</td>
<td>0.72</td>
</tr>
<tr>
<td><span class="math inline">E</span></td>
<td>0.8</td>
<td>-0.95</td>
<td>0.72</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p><br></p>
<h3 id="problem-7.1">Problem 7.1</h3>
<p>Albert wants to start with a simple model: fitting only a single
feature to obtain the true energy (i.e. <span class="math inline">y =
w_0+w_1 x</span>). Which feature should he choose as <span
class="math inline">x</span> to get the lowest mean square error?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">A</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">B</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">C</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-7.2">Problem 7.2</h3>
<p>Albert wants to add another feature to his linear regression in part
(a) to further boost the model’s performance. (i.e. <span
class="math inline">y = w_0 + w_1 x + w_2 x_2</span>) Which feature
should he choose as <span class="math inline">x_2</span> to make
additional improvements?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">A</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">B</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">C</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-7.3">Problem 7.3</h3>
<p>Albert further refines his algorithm by fitting a prediction rule of
the form: <span class="math display">\begin{aligned}
H(A,B,C) = w_0 + w_1 \cdot A\cdot C + w_2 \cdot B^{C-7}
\end{aligned}</span></p>
<p>Given this prediction rule, what are the dimensions of the design
matrix <span class="math inline">X</span>?</p>
<p><span class="math display">
\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
\end{bmatrix}_{r \times c}
</span></p>
<p>So, what are <span class="math inline">r</span> and <span
class="math inline">c</span> in <span class="math inline">r \text{ rows}
\times c \text{ columns}</span>?</p>
<p><br></p>
<!-- Commented out the last subproblem, because the solution is weird?? Need to rework. -->
<!-- <br>

### Problem 7.4



Now Albert solves the normal equations and finds the solution to be:
$$\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^* \\ w_2^*  \end{bmatrix}$$
To improve on this result, Albert decides to modify his design matrix
with the following steps:

1.  Add 1 to every entry to the first column

2.  Swap the second and the third column

Let $X_a$ be the modified design matrix. Let
$\vec{w_a}^* = (X_a^TX_a)^{-1}X_a^T\vec{y}$.

Express the components
$\vec{w_a}^*$ in terms of $w_0^*, w_1^*, w_2^*$, which were the
components of $\vec{w}^*$.

$$\vec{w_a}^* = \begin{bmatrix} ? \\ ? \\ ? \\ \end{bmatrix}$$







    
<br> -->
<hr />
<h2 id="problem-8">Problem 8</h2>
<p>Note that we have two simplified closed form expressions for the
estimated slope <span class="math inline">w</span> in simple linear
regression that you have already seen in discussions and lectures:</p>
<p><span class="math display">
\begin{align*}
    w &amp;= \frac{\sum_i (x_i - \overline{x}) y_i}{\sum_i (x_i -
\overline{x})^2} \\ \\
    w &amp;= \frac{\sum_i (y_i - \overline{y}) x_i }{\sum_i (x_i -
\overline{x})^2}
\end{align*}
</span></p>
<p>where we have dataset <span class="math inline">D = [(x_1,y_1),
\ldots, (x_n,y_n)]</span> and sample means   <span
class="math inline">\overline{x} = {1 \over n} \sum_{i} x_i, \quad
\overline{y} = {1 \over n} \sum_{i} y_i</span>. Without further
explanation, <span class="math inline">\sum_i</span> means <span
class="math inline">\sum_{i=1}^n</span></p>
<p><br></p>
<h3 id="problem-8.1">Problem 8.1</h3>
<p>Are (<span class="math inline">1</span>) and (<span
class="math inline">2</span>) equivalent? That is, is the following
equality true? Prove or disprove it. <span class="math display">\sum_i
(x_i - \overline{x}) y_i = \sum_i (y_i - \overline{y}) x_i</span></p>
<p><br></p>
<h3 id="problem-8.2">Problem 8.2</h3>
<p>True or False: If the dataset shifted right by a constant distance
<span class="math inline">a</span>, that is, we have the new dataset
<span class="math inline">D_a = (x_1 + a,y_1), \ldots, (x_n +
a,y_n)</span>, then will the estimated slope <span
class="math inline">w</span> change or not?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<h3 id="problem-8.3">Problem 8.3</h3>
<p>True or False: If the dataset shifted up by a constant distance <span
class="math inline">b</span>, that is, we have the new dataset <span
class="math inline">D_b = [(x_1,y_1 + b), \ldots, (x_n,y_n + b)]</span>,
then will the estimated slope <span class="math inline">w</span> change
or not?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> True</p></li>
<li><p><input type="radio" disabled="" /> False</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-9">Problem 9</h2>
<p>Consider a dataset that consists of <span class="math inline">y_1,
\cdots, y_n</span>. In class, we used calculus to minimize mean squared
error, <span class="math inline">R_{sq}(h) = \frac{1}{n} \sum_{i = 1}^n
(h - y_i)^2</span>. In this problem, we want you to apply the same
approach to a slightly different loss function defined below: <span
class="math display">L_{\text{midterm}}(y,h)=(\alpha y - h)^2+\lambda
h</span></p>
<p><br></p>
<h3 id="problem-9.1">Problem 9.1</h3>
<p>Write down the empiricial risk <span
class="math inline">R_{\text{midterm}}(h)</span> by using the above loss
function.</p>
<p><br></p>
<h3 id="problem-9.2">Problem 9.2</h3>
<p>The mean of dataset is <span class="math inline">\bar{y}</span>, i.e.
<span class="math inline">\bar{y} = \frac{1}{n} \sum_{i = 1}^n
y_i</span>. Find <span class="math inline">h^*</span> that minimizes
<span class="math inline">R_{\text{midterm}}(h)</span> using calculus.
Your result should be in terms of <span
class="math inline">\bar{y}</span>, <span
class="math inline">\alpha</span> and <span
class="math inline">\lambda</span>.</p>
<p><br></p>
<hr />
<h2 id="problem-10">Problem 10</h2>
<p>For a given dataset <span class="math inline">\{y_1, y_2, \dots,
y_n\}</span>, let <span class="math inline">M_{abs}(h)</span> represent
the <strong>median</strong> absolute error of the constant prediction
<span class="math inline">h</span> on that dataset (as opposed to the
mean absolute error <span class="math inline">R_{abs}(h)</span>).</p>
<p><br></p>
<h3 id="problem-10.1">Problem 10.1</h3>
<p>For the dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, what is <span class="math inline">M_{abs}(9)</span>?</p>
<p><br></p>
<h3 id="problem-10.2">Problem 10.2</h3>
<p>For the same dataset <span class="math inline">\{4, 9, 10, 14,
15\}</span>, find another integer <span class="math inline">h</span>
such that <span class="math inline">M_{abs}(9) = M_{abs}(h)</span>.</p>
<p><br></p>
<h3 id="problem-10.3">Problem 10.3</h3>
<p>Based on your answers to parts (a) and (b), discuss in <strong>at
most two sentences</strong> what is problematic about using the median
absolute error to make predictions.</p>
<p><br></p>
<hr />
<h2 id="problem-11">Problem 11</h2>
<p>Billy’s aunt owns a jewellery store, and gives him data on <span
class="math inline">5000</span> of the diamonds in her store. For each
diamond, we have:</p>
<ul>
<li><strong>carat</strong>: the weight of the diamond, in carats</li>
<li><strong>length</strong>: the length of the diamond, in
centimeters</li>
<li><strong>width</strong>: the width of the diamond, in
centimeters</li>
<li><strong>price</strong>: the value of the diamond, in dollars</li>
</ul>
<p>The first 5 rows of the 5000-row dataset are shown below:</p>
<div>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 27%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr>
<th>carat   </th>
<th>length   </th>
<th>width   </th>
<th>price   </th>
</tr>
</thead>
<tbody>
<tr>
<td>0.40</td>
<td>4.81</td>
<td>4.76</td>
<td>1323</td>
</tr>
<tr>
<td>1.04</td>
<td>6.58</td>
<td>6.53</td>
<td>5102</td>
</tr>
<tr>
<td>0.40</td>
<td>4.74</td>
<td>4.76</td>
<td>696</td>
</tr>
<tr>
<td>0.40</td>
<td>4.67</td>
<td>4.65</td>
<td>798</td>
</tr>
<tr>
<td>0.50</td>
<td>4.90</td>
<td>4.95</td>
<td>987</td>
</tr>
</tbody>
</table>
</div>
<p><br> Billy has enlisted our help in predicting the price of a diamond
given various other features.</p>
<p><br></p>
<h3 id="problem-11.1">Problem 11.1</h3>
<p>Suppose we want to fit a linear prediction rule that uses two
features, carat and length, to predict price. Specifically, our
prediction rule will be of the form</p>
<p><span class="math display">\text{predicted price} = w_0 + w_1 \cdot
\text{carat} + w_2 \cdot \text{length}</span></p>
<p><br></p>
<p>We will use least squares to find <span class="math inline">\vec{w}^*
= \begin{bmatrix} w_0^* \\ w_1^* \\ w_2^* \end{bmatrix}</span>.</p>
<p>Write out the first 5 rows of the design matrix, <span
class="math inline">X</span>. Your matrix should not have any variables
in it.</p>
<p><br></p>
<h3 id="problem-11.2">Problem 11.2</h3>
<p>Suppose the optimal parameter vector <span
class="math inline">\vec{w}^*</span> is given by</p>
<p><span class="math display">\vec{w}^* = \begin{bmatrix} 2000 \\ 10000
\\ -1000 \end{bmatrix}</span></p>
<p>What is the predicted price of a diamond with 0.65 carats and a
length of 4 centimeters? Show your work.</p>
<p><br></p>
<h3 id="problem-11.3">Problem 11.3</h3>
<p>Suppose <span class="math inline">\vec{e} = \begin{bmatrix} e_1 \\
e_2 \\ ... \\ e_n \end{bmatrix}</span> is the error/residual vector,
defined as</p>
<p><span class="math display">\vec{e} = \vec{y} - X \vec{w}^*</span></p>
<p>where <span class="math inline">\vec{y}</span> is the observation
vector containing the prices for each diamond.</p>
<p>For each of the following quantities, state whether they are
guaranteed to be equal to 0 the scalar, <span
class="math inline">\vec{0}</span> the vector of all 0s, or neither. No
justification is necessary.</p>
<ul>
<li><span class="math inline">\sum_{i = 1}^n e_i</span></li>
<li><span class="math inline">|| \vec{y} - X \vec{w}^* ||^2</span></li>
<li><span class="math inline">X^TX \vec{w}^*</span></li>
<li><span class="math inline">2X^TX \vec{w}^* - 2X^T\vec{y}</span></li>
</ul>
<p><br></p>
<h3 id="problem-11.4">Problem 11.4</h3>
<p>Suppose we introduce two more features:</p>
<ul>
<li>width alone, and</li>
<li>area, which is defined as length times width</li>
</ul>
<p>Suppose we also decide to remove the intercept term of our prediction
rule. With all of these changes, our prediction rule is now</p>
<p><span class="math display">\text{predicted price} = w_1 \cdot
\text{carat} + w_2 \cdot \text{length} + w_3 \cdot \text{width} + w_4
\cdot (\text{length} \cdot \text{width}) </span></p>
<ul>
<li>Write out just the first 2 rows of the design matrix <span
class="math inline">X</span> for this new prediction rule. You do
<strong>not</strong> need to simplify the numbers in your matrix, it is
fine if they involve the multiplication symbol.</li>
<li>Is the optimal coefficient for carat, <span
class="math inline">w_1^*</span>, for this new prediction rule
guaranteed to be equal to 10000, the optimal coefficient for carat in
our original prediction rule? No justification is necessary.</li>
</ul>
<p><br></p>
<hr />
<h2 id="problem-12">Problem 12</h2>
<p>Let <span class="math inline">X</span> be a design matrix with 4
columns, such that the first column is a column of all <span
class="math inline">1</span>s. Let <span
class="math inline">\vec{y}</span> be an observation vector. Let <span
class="math inline">\vec{w}^* = (X^TX)^{-1}X^T\vec{y}.</span> We’ll name
the components of <span class="math inline">\vec{w}^*</span> as
follows:</p>
<p><span class="math display">\vec{w}^* = \begin{bmatrix} w_0^* \\ w_1^*
\\ w_2^* \\ w_3^* \end{bmatrix}</span></p>
<p>In this problem, we’ll consider various modifications to the design
matrix and see how they affect the solution to the normal equations.</p>
<p><br></p>
<h3 id="problem-12.1">Problem 12.1</h3>
<p>Let <span class="math inline">X_a</span> be the design matrix that
comes from <strong>interchanging the first two columns</strong> of <span
class="math inline">X</span>. Let <span class="math inline">\vec{w_a}^*
= (X_a^TX_a)^{-1}X_a^T\vec{y}</span>. Express the components <span
class="math inline">\vec{w_a}^*</span> in terms of <span
class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span> (which were the components of <span
class="math inline">\vec{w}^*</span>).</p>
<p><br></p>
<h3 id="problem-12.2">Problem 12.2</h3>
<p>Let <span class="math inline">X_b</span> be the design matrix that
comes from <strong>adding one to each entry of the first column</strong>
of <span class="math inline">X</span>. Let <span
class="math inline">\vec{w_b}^* = (X_b^TX_b)^{-1}X_b^T\vec{y}</span>.
Express the components <span class="math inline">\vec{w_b}^*</span> in
terms of <span class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span> (which were the components of <span
class="math inline">\vec{w}^*</span>).</p>
<p><br></p>
<h3 id="problem-12.3">Problem 12.3</h3>
<p>Let <span class="math inline">X_c</span> be the design matrix that
comes from <strong>adding one to each entry of the third column</strong>
of <span class="math inline">X</span>. Let <span
class="math inline">\vec{w_c}^* = (X_c^TX_c)^{-1}X_c^T\vec{y}</span>.
Express the components <span class="math inline">\vec{w_c}^*</span> in
terms of <span class="math inline">w_0^*, w_1^*, w_2^*</span>, and <span
class="math inline">w_3^*</span>, which were the components of <span
class="math inline">\vec{w}^*</span>.</p>
<p><br></p>
<hr />
<h2 id="problem-13">Problem 13</h2>
<p>Suppose we want to predict how long it takes to run a Jupyter
notebook on Datahub. For 100 different Jupyter notebooks, we collect the
following 5 pieces of information:</p>
<ul>
<li><p><strong>cells</strong>: number of cells in the notebook</p></li>
<li><p><strong>lines</strong>: number of lines of code</p></li>
<li><p><strong>max iterations</strong>: largest number of iterations in
any loop in the notebook, or 1 if there are no loops</p></li>
<li><p><strong>variables</strong>: number of variables defined in the
notebook</p></li>
<li><p><strong>runtime</strong>: number of seconds for the notebook to
run on Datahub</p></li>
</ul>
<p>Then we use multiple regression to fit a prediction rule of the form
<span class="math display">H(\text{cells, lines, max iterations,
variables}) =  w_0 + w_1 \cdot \text{cells} \cdot \text{lines} + w_2
\cdot (\text{max iterations})^{\text{variables} - 10}</span></p>
<p><br></p>
<h3 id="problem-13.1">Problem 13.1</h3>
<p>What are the dimensions of the design matrix <span
class="math inline">X</span>?</p>
<p><span class="math display">
\begin{bmatrix}
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
&amp; &amp; &amp; \\
\end{bmatrix}_{r \times c}
</span></p>
<p>So, what should <span class="math inline">r</span> and <span
class="math inline">c</span> be for: <span class="math inline">r</span>
rows <span class="math inline">\times</span> <span
class="math inline">c</span> columns.</p>
<p><br></p>
<h3 id="problem-13.2">Problem 13.2</h3>
<p>In <strong>one sentence</strong>, what does the entry in row 3,
column 2 of the design matrix X represent? (Count rows and columns
starting at 1, not 0).</p>
<p><br></p>
<hr />
<h2 id="problem-14">Problem 14</h2>
<p>Consider the vectors <span class="math inline">\vec{u}</span> and
<span class="math inline">\vec{v}</span>, defined below.</p>
<p><span class="math display">\vec{u} = \begin{bmatrix} 1 \\ 0 \\ 0
\end{bmatrix} \qquad \vec{v} = \begin{bmatrix} 0 \\ 1 \\ 1
\end{bmatrix}</span></p>
<p>We define <span class="math inline">X \in \mathbb{R}^{3 \times
2}</span> to be the matrix whose first column is <span
class="math inline">\vec u</span> and whose second column is <span
class="math inline">\vec v</span>.</p>
<p><br></p>
<h3 id="problem-14.1">Problem 14.1</h3>
<p>In this part only, let <span class="math inline">\vec{y} =
\begin{bmatrix} -1 \\ k \\ 252 \end{bmatrix}</span>.</p>
<p>Find a scalar <span class="math inline">k</span> such that <span
class="math inline">\vec{y}</span> is in <span
class="math inline">\text{span}(\vec u, \vec v)</span>. Give your answer
as a constant with no variables.</p>
<p><br></p>
<h3 id="problem-14.2">Problem 14.2</h3>
<p>Show that: <span class="math display">(X^TX)^{-1}X^T =
\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; \frac{1}{2} &amp;
\frac{1}{2} \end{bmatrix}</span></p>
<p><em>Hint: If <span class="math inline">A = \begin{bmatrix} a_1 &amp;
0 \\ 0 &amp; a_2 \end{bmatrix}</span>, then <span
class="math inline">A^{-1} = \begin{bmatrix} \frac{1}{a_1} &amp; 0 \\ 0
&amp; \frac{1}{a_2} \end{bmatrix}</span>.</em></p>
<p><br></p>
<h3 id="problem-14.3">Problem 14.3</h3>
<p>In parts (c) and (d) only, let <span class="math inline">\vec{y} =
\begin{bmatrix} 4 \\ 2 \\ 8 \end{bmatrix}</span>.</p>
<p>Find scalars <span class="math inline">a</span> and <span
class="math inline">b</span> such that <span class="math inline">a \vec
u + b \vec v</span> is the vector in <span
class="math inline">\text{span}(\vec u, \vec v)</span> that is as close
to <span class="math inline">\vec{y}</span> as possible. Give your
answers as constants with no variables.</p>
<p><br></p>
<h3 id="problem-14.4">Problem 14.4</h3>
<p>Let <span class="math inline">\vec{e} = \vec{y} - (a \vec u + b \vec
v)</span>, where <span class="math inline">a</span> and <span
class="math inline">b</span> are the values you found in part (c).</p>
<p>What is <span class="math inline">\lVert \vec{e} \rVert</span>?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> <span class="math inline">0</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">3 \sqrt{2}</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">4 \sqrt{2}</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">6 \sqrt{2}</span></p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">2\sqrt{21}</span></p></li>
</ul>
<p><br></p>
<h3 id="problem-14.5">Problem 14.5</h3>
<p>Is it true that, for any vector <span class="math inline">\vec{y} \in
\mathbb{R}^3</span>, we can find scalars <span
class="math inline">c</span> and <span class="math inline">d</span> such
that the sum of the entries in the vector <span
class="math inline">\vec{y} - (c \vec u + d \vec v)</span> is 0?</p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> Yes, because <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> are linearly independent.</p></li>
<li><p><input type="radio" disabled="" /> Yes, because <span class="math inline">\vec{u}</span> and <span class="math inline">\vec{v}</span> are orthogonal.</p></li>
<li><p><input type="radio" disabled="" /> Yes, but for a reason that isn’t listed here.</p></li>
<li><p><input type="radio" disabled="" /> No, because <span class="math inline">\vec{y}</span> is not
necessarily in</p></li>
<li><p><input type="radio" disabled="" /> No, because neither <span class="math inline">\vec{u}</span> nor
<span class="math inline">\vec{v}</span> is equal to the vector</p></li>
<li><p><input type="radio" disabled="" /> No, but for a reason that isn’t listed here.</p></li>
</ul>
<p><br></p>
<h3 id="problem-14.6">Problem 14.6</h3>
<p>Suppose that <span class="math inline">Q \in \mathbb{R}^{100 \times
12}</span>, <span class="math inline">\vec{s} \in
\mathbb{R}^{100}</span>, and <span class="math inline">\vec{f} \in
\mathbb{R}^{12}</span>. What are the dimensions of the following
product?</p>
<p><span class="math display">\vec{s}^T Q \vec{f}</span></p>
<ul class="task-list">
<li><p><input type="radio" disabled="" /> scalar</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">12 \times 1</span> vector</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">100 \times 1</span> vector</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">100 \times 12</span> matrix</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">12 \times 12</span> matrix</p></li>
<li><p><input type="radio" disabled="" /> <span class="math inline">12 \times 100</span> matrix</p></li>
<li><p><input type="radio" disabled="" /> undefined</p></li>
</ul>
<p><br></p>
<hr />
<h2 id="section"><span class="math display"> </span></h2>
<h4
id="feedback-find-an-error-still-confused-have-a-suggestion-let-us-know-here.">👋
Feedback: Find an error? Still confused? Have a suggestion? Let us know
<a href="https://forms.gle/xK4DpWXh9rq8AKP37">here</a>.</h4>
<hr />
</body>
</html>
