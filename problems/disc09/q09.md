# BEGIN PROB

Suppose we want to fit a multiple linear regression model (using squared
loss) that predicts the number of ingredients in a product given its
price and various other information.

From the Data Overview page, we know that there are **6** different
**types** of products. Assume in this question that there are **20**
different product **brands**. Consider the models defined in the table
below.

| **Model Name** | **Intercept** | **Price** | **Type**                  | **Brand**                     |
|----------------|---------------|-----------|---------------------------|-------------------------------|
| **Model A**    |     Yes       |    Yes    |    No                     |  One hot encoded              |
|                |               |           |                           |  **without** `drop="first"`   |
|                |               |           |                           |                               |
| **Model B**    |     Yes       |    Yes    |    No                     |  No                           |
|                |               |           |                           |                               |
| **Model C**    |    Yes        |    Yes    |   One hot encoded         |                               |
|                |               |           | **without** `drop="first"`|                               |
|                |               |           |                           |                               |
| **Model D**    | No            | Yes       | One hot encoded           | One hot encoded               |
|                |               |           | **with** `drop="first"`   | **with** `drop="first"`       |
|                |               |           |                           |                               |
| **Model E**    | No            | Yes       | One hot encoded           | One hot encoded               |
|                |               |           | **with** `drop="first"`   | **without** `drop="first"`    |

For instance, Model A above includes an intercept term, price as a
feature, one hot encodes brand names, and doesn't use `drop="first"` as
an argument to `OneHotEncoder` in `sklearn`.

In parts 1 through 3, you are given a model. For each model
provided, state the **number** of columns and the rank (i.e. number of
linearly independent columns) of the design matrix, $X$. Some of part
1 is already done for you as an example.

# BEGIN SUBPROB

**Model A**

Number of columns in $X$:  
Rank of $X$: **21**

# BEGIN SOLUTION
**Answer**: 

Number of columns in $X$: **22**

Model A includes an intercept, the price feature, and a one-hot encoding of the 20 brands without dropping the first category. The intercept adds 1 column, the price adds 1 column, and the one-hot encoding for brands adds 20 columns. The rank is **21** because one of the brand categories can be predicted by the others (i.e., the columns are linearly dependent). 

The rank is 21 because when encoding categorical variables (such as brands), dropping the first category avoids introducing perfect multicollinearity. By not dropping a category, we increase the rank by 1, but this still maintains linear dependence among the columns.



# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**Model B**

Number of columns in $X$:  
Rank of $X$:  

# BEGIN SOLUTION
**Answer**: 

Number of columns in $X$: **2**  
Rank of $X$: **2**

Model B includes an intercept and the price feature, but no one-hot encoding for the brands or product types. The intercept and price are linearly independent, resulting in both the number of columns and the rank being 2.


# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**Model C**

Number of columns in $X$:  
Rank of $X$:  

# BEGIN SOLUTION
**Answer**: 

Number of columns in $X$: **8**  
Rank of $X$: **7**

Model C includes an intercept, the price feature, and a one-hot encoding of the 6 product types without dropping the first category. The intercept adds 1 column, the price adds 1 column, and the one-hot encoding for the product types adds 6 columns. However, one column is linearly dependent because we didn't drop one of the one-hot encoded columns, reducing the rank to 7.

This happens because if we have 6 product types, one of the columns can always be written as a linear combination of the other five. Thus, we lose one degree of freedom in the matrix, and the rank is 7.


# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which of the following models are **NOT guaranteed** to have residuals
that sum to 0?

*Hint: Remember, the residuals of a fit model are the differences
between actual and predicted $y$-values, among the data points in the
training set.*

[ ] Model A
[ ] Model B
[ ] Model C
[ ] Model D
[ ] Model E

# BEGIN SOLUTION
**Answer**: Model D

For residuals to sum to zero in a linear regression model, the design matrix  must include either an intercept term (Models A - C) or equivalent redundancy in the encoded variables to act as a substitute for the intercept (Model E). Models that lack both an intercept and equivalent redundancy, such as Model D, are not guaranteed to have residuals sum to zero.

For a linear model, the residual vector $\vec{e}$ is chosen such that it is orthogonal to the column space of the design matrix $X$. If one of the columns of $X$ is a column of all ones (as in the case with an intercept term), it ensures that the sum of the residuals is zero. This is because the error vector  $\vec{e}$ is orthogonal to the span of $X$, and hence $X^T \vec{e} = 0$. If there isnâ€™t a column of all ones, but we can create such a column using a linear combination of the other columns in $X$, the residuals will still sum to zero. Model D lacks this structure, so the residuals are not guaranteed to sum to zero.

In contrast, one-hot encoding without dropping any category can behave like an intercept because it introduces a column for each category, allowing the model to adjust its predictions as if it had an intercept term. The sum of the indicator variables in such a setup equals 1 for every row, creating a similar effect to an intercept, which is why residuals may sum to zero in these cases. 



# END SOLUTION

# END SUBPROB

# END PROB