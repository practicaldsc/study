# BEGIN PROB

Suppose we'd like to predict the number of minutes a delivery will take
($y$) as a function of distance ($x$). To do so, we look to our dataset
of $n$ deliveries, $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, and fit
two simple linear models:

-   $F(x_i) = a_0 + a_1 x_i$, where:
    $$a_1 = r \frac{\sigma_y}{\sigma_x}, \qquad a_0 = \bar{y} - r \frac{\sigma_y}{\sigma_x} \bar{x}$$

    Here, $r$ is the correlation coefficient between $x$ and $y$,
    $\bar{x}$ and $\bar{y}$ are the means of $x$ and $y$, respectively,
    and $\sigma_x$ and $\sigma_y$ are the standard deviations of $x$ and
    $y$, respectively.

-   $G(x_i) = b_0 + b_1 x_i$, where $b_0$ and $b_1$ are chosen such that
    the line $G(x_i) = b_0 + b_1 x_i$ minimizes **mean absolute error**
    on the dataset. Assume that no other line minimizes mean absolute
    error on the dataset, i.e. that the values of $b_0$ and $b_1$ are
    unique.

# BEGIN SUBPROB

Fill in the :
$$\displaystyle \sum_{i = 1}^n (y_i - G(x_i))^2 \:\:\: \boxed{???} \:\:\: \sum_{i = 1}^n (y_i - F(x_i))^2$$
( ) $\geq$ ( ) $>$ ( ) $=$ ( ) $<$ ( ) $\leq$ ( ) Impossible to tell

# BEGIN SOLUTION

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Fill in the :
$$\displaystyle \left( \sum_{i = 1}^n \big| y_i - G(x_i) \big| \right)^2 \:\:\: \boxed{???} \:\:\: \left( \sum_{i = 1}^n \big| y_i - F(x_i) \big| \right)^2$$
( ) $\geq$ ( ) $>$ ( ) $=$ ( ) $<$ ( ) $\leq$ ( ) Impossible to tell

# BEGIN SOLUTION

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Below, we've drawn both lines, $F$, and $G$, along with a scatter plot
of the original $n$ deliveries.

::: center
![image](midterm-images/regression.png){width="50%"}
:::

Which line corresponds to line **$F$**? ( ) Line 1 ( ) Line 2

# BEGIN SOLUTION

# END SOLUTION

# END SUBPROB

# END PROB