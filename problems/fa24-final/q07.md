# BEGIN PROB

Consider a dataset of $n$ values, $y_1, y_2, ..., y_n$, all of which are
**positive**. We want to fit a constant model, $H(x) = h$, to the data.

Let $h_p^*$ be the optimal constant prediction that minimizes average
degree-$p$ loss, $R_p(h)$, defined below.

$$R_p(h) = \frac{1}{n} \sum_{i = 1}^n | y_i - h |^p$$

For example, $h_2^*$ is the optimal constant prediction that minimizes
$\displaystyle R_2(h) = \frac{1}{n} \sum_{i = 1}^n |y_i - h|^2$.

In each of the parts below, determine the value of the quantity
provided. By "the data\", we are referring to $y_1, y_2, ..., y_n$.

# BEGIN SUBPROB

$h_0^*$

( ) The standard deviation of the data  
( ) The variance of the data  
( ) The mean of the data  
( ) The median of the data  
( ) The midrange of the data, $\frac{y_\text{min} + y_\text{max}}{2}$  
( ) The mode of the data  
( ) None of the above  

# BEGIN SOLUTION
**Answer**: The mode of the data

$R_0(h)$ counts the number of incorrect predictions for each prediction, so choosing the most frequent value (mode) minimizes this count.

**Note**: This was our intended answer, as the idea here was to mirror 0-1 loss from discussion (and briefly mentioned in lecture). 0-1 loss is defined as:

$$L_{0-1}(y_i, h) = \begin{cases} 0 & \text{if } y_i = h \\ 1 & \text{if } y_i \neq h \end{cases}$$

But, we posed the question in a way that's a little too vague, as $$0^0$$ is undefined, and we need it to be $$0$$ for this interpretation to hold. So, we gave everyone credit for this part.

<average>100</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

$h_1^*$

( ) The standard deviation of the data  
( ) The variance of the data  
( ) The mean of the data  
( ) The median of the data  
( ) The midrange of the data, $\frac{y_\text{min} + y_\text{max}}{2}$  
( ) The mode of the data  
( ) None of the above  

# BEGIN SOLUTION
**Answer**: The median of the data 

As seen in lecture, the minimizer of empirical risk for the constant model when using absolute loss is the median.

<average>87</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

$R_1(h_1^*)$

( ) The standard deviation of the data  
( ) The variance of the data  
( ) The mean of the data  
( ) The median of the data  
( ) The midrange of the data, $\frac{y_\text{min} + y_\text{max}}{2}$  
( ) The mode of the data  
( ) None of the above  

# BEGIN SOLUTION
**Answer**: None of the above

As we discussed in the previous subpart, $h_1^*$, the minimizer of mean absolute error, is the median. So, $R_1(h_1^*)$ is the mean absolute difference (or deviation) between all values and the median. This is not any of the answer choices, and in general, is not a quantity we studied in detail in class.

$R_1(h_1^*)$ represents the sum of absolute deviations from the median divided by $n$:

$$R_1(h_1^*) = \frac{1}{n} \sum_{i=1}^n |y_i - h_1^*|$$

<average>44</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

$h_2^*$

( ) The standard deviation of the data  
( ) The variance of the data  
( ) The mean of the data  
( ) The median of the data  
( ) The midrange of the data, $\frac{y_\text{min} + y_\text{max}}{2}$  
( ) The mode of the data  
( ) None of the above  

# BEGIN SOLUTION
**Answer**: The mean of the data

The minimizer of empirical risk for the constant model when using squared loss is the mean.

<average>81</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

$R_2(h_2^*)$

( ) The standard deviation of the data  
( ) The variance of the data  
( ) The mean of the data  
( ) The median of the data  
( ) The midrange of the data, $\frac{y_\text{min} + y_\text{max}}{2}$  
( ) The mode of the data  
( ) None of the above  

# BEGIN SOLUTION
**Answer**: The variance of the data

$R_2(h)$ represents the mean squared error of the constant prediction, h. In the previous part, we discussed that $h_2^*$, the minimizer of $R_2(h)$, is the mean, $\bar{y}$. So, $R_2(h_2^*)$ is then:

$$R_2(h_2^*) = \frac{1}{n} \sum_{i = 1}^n (y_i - h_2^*)^2 = \frac{1}{n} \sum_{i = 1}^n (y_i - \bar{y})^2$$

This is the definition of the variance of y.

<average>70</average>

# END SOLUTION

# END SUBPROB

Now, suppose we want to find the optimal constant prediction,
$h_\text{U}^*$, using the "Ulta\" loss function, defined below.

$$L_U(y_i, h) = y_i (y_i - h)^2$$

# BEGIN SUBPROB

To find $h_\text{U}^*$, suppose we minimize average Ulta loss
(with no regularization). How does $h_\text{U}^*$ compare to the mean of
the data, $M$?

( ) $h_\text{U}^* > M$
( ) $h_\text{U}^* \geq M$
( ) $h_\text{U}^* = M$
( ) $h_\text{U}^* \leq M$
( ) $h_\text{U}^* < M$

# BEGIN SOLUTION
**Answer**: $h_\text{U}^* \geq M$

Minimizing the average Ulta loss means minimizing the empirical risk:

$$ R_U(h) = \frac{1}{n} \sum_{i = 1}^n y_i (y_i - h)^2. $$

This resembles minimizing mean squared error, except each $y_i$ is given a weight of $y_i$. Since larger $y_i$ values contribute more to the loss, the minimizer is pulled toward higher values to reduce their impact. This results in $h_\text{U}^*$ being **greater than** the mean of the data.


<average>42</average>

# END SOLUTION

# END SUBPROB

Now, to find the optimal constant prediction, we will instead minimize
**regularized** average Ulta loss, $R_\lambda(h)$, where $\lambda$ is a
non-negative regularization hyperparameter:

$$R_\lambda(h) = \left( \frac{1}{n} \sum_{i = 1}^n y_i (y_i - h)^2 \right) + \lambda h^2$$

It can be shown that
$\displaystyle \frac{\partial R_\lambda(h)}{\partial h}$, the derivative
of $R_\lambda(h)$ with respect to $h$, is:

$$\frac{\partial R_\lambda(h)}{\partial h} = -2 \left( \frac{1}{n} \sum_{i = 1}^n y_i (y_i - h) - \lambda h \right)$$

# BEGIN SUBPROB

[ðŸŽ¥ Walkthrough Available](https://www.loom.com/share/3cfce64b4e424f1e9119aa787b25b005?sid=ceb5fa4d-b7a3-48ad-bbc4-43e437b739a2)

Find $h^*$, the constant prediction that minimizes $R_\lambda(h)$. Show
your work, and put a $\boxed{\text{box}}$ around your final answer, which should be an
**expression in terms of $y_i$, $n$, and/or $\lambda$**.

# BEGIN SOLUTION
**Answer**: $h^* = \frac{\sum_{i = 1}^n y_i^2}{\sum_{i = 1}^n y_i + n\lambda}$

<center><iframe width="800" height="450" src="https://www.loom.com/embed/3cfce64b4e424f1e9119aa787b25b005?sid=85e54d2a-ea46-4eb7-85f6-9b5837998f61" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></center>

To minimize the regularized average Ulta loss, we solve for $h$ by setting  
$$ \frac{\partial R_\lambda(h)}{\partial h} = 0. $$  

Step 1: Compute the derivative and set it to zero  

$$ \frac{\partial R_\lambda(h)}{\partial h} = -2 \left( \frac{1}{n} \sum_{i = 1}^n y_i (y_i - h) \right) + 2\lambda h = 0 $$

Step 2: Expand and simplify  

$$ \frac{1}{n} \sum_{i = 1}^n y_i^2 - \frac{1}{n} \sum_{i = 1}^n y_i h - \lambda h = 0 $$  

Rearrange terms:  

$$ \frac{1}{n} \sum_{i = 1}^n y_i^2 = h \left( \frac{1}{n} \sum_{i = 1}^n y_i + \lambda \right) $$  

Step 3: Solve for $h$  

Divide both sides by $\frac{1}{n} \sum_{i = 1}^n y_i + \lambda:$ 

$$ h^* = \frac{\frac{1}{n} \sum_{i = 1}^n y_i^2}{\frac{1}{n} \sum_{i = 1}^n y_i + \lambda} $$  

Step 4: Multiply by $\frac{n}{n}$  

$$ h^* = \frac{\sum_{i = 1}^n y_i^2}{\sum_{i = 1}^n y_i + n\lambda} $$  

<average>71</average>

# END SOLUTION

# END SUBPROB

# END PROB
