# BEGIN PROB

Suppose we want to create polynomial features and use ridge regression
(i.e. minimize mean squared error with $L_2$ regularization) to fit a
linear model that predicts the number of ingredients in a product given
its price.

To choose the polynomial degree and regularization hyperparameter, we
use cross-validation through `GridSearchCV` in `sklearn` using the code
below.

```python
    searcher = GridSearchCV(
        make_pipeline(PolynomialFeatures(include_bias=False), 
                      Ridge()),
        
        param_grid={"polynomialfeatures__degree": np.arange(1, D + 1), 
                    "ridge__alpha": 2 ** np.arange(1, L + 1)},

        cv=K # K-fold cross-validation.
    ) 
    searcher.fit(X_train, y_train)
```
Assume that there are $N$ rows in `X_train`, where $N$ is a multiple of
$K$ (the number of folds used for cross-validation).

In each of the parts below, give your answer as an **expression
involving $D$, $L$, $K$, and/or $N$**. Part (a) is done for you as an
example.

# BEGIN SUBPROB

How many combinations of hyperparameters are being considered?
**Answer**: $LD$

# BEGIN SOLUTION
**Answer**: $LD$

There are $L$ possible values of $\lambda$ and $D$ possible polynomial degrees, giving $LD$ total combinations of hyperparameters.

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**Each time** a model is trained, how many points are being
used to train the model?

# BEGIN SOLUTION
**Answer**: $N \cdot \frac{K-1}{K}$

In $K$-fold cross-validation, the data is split into $K$ folds. Each time a model is trained, one fold is used for validation, and the remaining $K-1$ folds are used for training. Since $N$ is evenly split among $K$ folds, each fold contains $\frac{N}{K}$ points. The training data, therefore, consists of $N - \frac{N}{K} = N \cdot \frac{K-1}{K}$ points.

<average>72</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**In total**, how many times are `X_train.iloc[1]` and
`X_train.iloc[-1]` **both** used for training a model **at the same
time**? Assume that these two points are in different folds.

# BEGIN SOLUTION
**Answer**: $LD \times (K-2)$

In K-fold cross-validation, `X_train.iloc[1]` and `X_train.iloc[-1]` are in different folds and can only be used together for training when neither of their respective folds is the validation set. This happens in $(K-2)$ folds for each combination of polynomial degree ($D$) and regularization hyperparameter ($L$). Since there are $L \times D$ total combinations in the grid search, the two points are used together for training $LD \times (K-2)$ times.

<average>53</average>

# END SOLUTION

# END SUBPROB

# END PROB