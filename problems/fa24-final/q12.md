# BEGIN PROB

Suppose we fit five different classifiers that predict whether a product
is designed for sensitive skin, given its price and number of
ingredients. In the five decision boundaries below, the gray-shaded
regions represent areas in which the classifier would predict that the
product **is** designed for sensitive skin (i.e. predict class 1).

<figure style="display: flex; flex-wrap: wrap; gap: 10px;">
    <div class="minipage">
        <img src="../assets/images/fa24-final/db1.png" width="400"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db2.png" width="400"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db3.png" width="400"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db4.png" width="400"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db5.png" width="400"/>
    </div>
</figure>

# BEGIN SUBPROB

Which model does Decision Boundary 1 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: Logistic Regression

Logistic regression creates a linear divide between classes. 

<average>93</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 2 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: Decision tree with $\text{max depth} = 15$

We know that Decision Boundaries 2 and 5 are decision trees, since the decision boundaries are parallel to the axes. Decision boundaries for decision trees are parallel to axes because the tree splits the feature space based on one feature at a time, creating partitions aligned with that feature's axis (e.g., "Price â‰¤ 100" results in a vertical split).

Decision Boundary 2 is more complicated than Decision Boundary 5, so we can assume there are more levels. 

<average>82</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 3 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: $k$-nearest neighbors with $k = 3$

$k$-nearest neighbors decision boundaries don't follow straight lines like logistic regression or decision trees. Thus, boundaries 3 and 4 are $k$-nearest neighbors. 

Boundary 3 seems to be more fit to the data than boundary 4, so we can assume that we used a decision tree with $\text{max depth} = 3$. 

For example, if in our training data we had 3 points in the group represented by the light-shaded region at $(300, 150)$, the decision boundary for $k$-nearest neighbors with $k = 3$ would be light-shaded at $(300, 150)$, whereas it may be dark-shaded for $k$-nearest neighbors with $k = 100$. 

<average>59</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 4 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: $k$-nearest neighbors with $k = 100$

$k$-nearest neighbors decision boundaries don't follow straight lines like logistic regression or decision trees. Thus, boundaries 3 and 4 are $k$-nearest neighbors. 

Boundary 3 seems to be less fit to the data than boundary 4, so we can assume that we used a decision tree with $\text{max depth} = 100$. 

<average>56</average>

# END SOLUTION

# END SUBPROB

# END PROB