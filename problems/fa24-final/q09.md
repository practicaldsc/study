# BEGIN PROB

Suppose we want to fit a multiple linear regression model (using squared
loss) that predicts the number of ingredients in a product given its
price and various other information.

From the Data Overview page, we know that there are **6** different
**types** of products. Assume in this question that there are **20**
different product **brands**. Consider the models defined in the table
below.

| **Model Name** | **Intercept** | **Price** | **Type**                  | **Brand**                     |
|-----------------|---------------|-----------|---------------------------|-------------------------------|
| **Model A**    | Yes           | Yes       | No                        | One hot encoded               |
|                |               |           |                           | **without** `drop="first"`    |
| **Model B**    | Yes           | Yes       | No                        | No                            |
| **Model C**    | Yes           | Yes       | One hot encoded           |                               |
|                |               |           | **without** `drop="first"`|                               |
| **Model D**    | No            | Yes       | One hot encoded           | One hot encoded               |
|                |               |           | **with** `drop="first"`   | **with** `drop="first"`       |
| **Model E**    | No            | Yes       | One hot encoded           | One hot encoded               |
|                |               |           | **with** `drop="first"`   | **without** `drop="first"`    |

For instance, Model A above includes an intercept term, price as a
feature, one hot encodes brand names, and doesn't use `drop="first"` as
an argument to `OneHotEncoder` in `sklearn`.

In parts (a) through (c), you are given a model. For each model
provided, state the **number** of columns and the rank (i.e. number of
linearly independent columns) of the design matrix, $X$. Some of part
(a) is already done for you as an example.

# BEGIN SUBPROB

**Model A**

Number of columns in $X$: Rank of $X$: **21**

# BEGIN SOLUTION
**Answer**: 

Columns: **22**

Model A includes an intercept, the price feature, and a one-hot encoding of the 20 brands without dropping the first category. The intercept adds 1 column, the price adds 1 column, and the one-hot encoding for brands adds 20 columns.

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**Model B**

Number of columns in $X$: Rank of $X$:

# BEGIN SOLUTION
**Answer**: 

Columns: **2**

Rank: **2**

Model B includes an intercept and the price feature, but no one-hot encoding for the brands or product types. The intercept and price are linearly independent, resulting in both the number of columns and the rank being 2.

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

**Model C**

Number of columns in $X$: Rank of $X$:

# BEGIN SOLUTION
**Answer**: 

Columns: **8**

Rank: **7**

Model C includes an intercept, the price feature, and a one-hot encoding of the 6 product types without dropping the first category. The intercept adds 1 column, the price adds 1 column, and the one-hot encoding for the product types adds 6 columns. However, one column is linearly dependent because we didn't drop one of the one-hot encoded columns, reducing the rank to 7.

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which of the following models are **NOT guaranteed** to have residuals
that sum to 0?

*Hint: Remember, the residuals of a fit model are the differences
between actual and predicted $y$-values, among the data points in the
training set.*

[ ] Model A
[ ] Model B
[ ] Model C
[ ] Model D
[ ] Model E

# BEGIN SOLUTION
**Answer**: Model D

For residuals to sum to zero in a linear regression model, the design matrix  must include either an intercept term (Models A - C) or equivalent redundancy in the encoded variables to act as a substitute for the intercept (Model E). Models that lack both an intercept and equivalent redundancy, such as Model D, are not guaranteed to have residuals sum to zero.

One-hot encoding without dropping any category can behave like an intercept because it introduces a column for each category, allowing the model to adjust its predictions as if it had an intercept term. The sum of the indicator variables in such a setup equals 1 for every row, creating a similar effect to an intercept, which is why residuals may sum to zero in these cases. 

# END SOLUTION

# END SUBPROB

# END PROB