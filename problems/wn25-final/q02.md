# BEGIN PROB

Suppose we'd like to build a regression model to predict the width of a
fish, given its height and weight.

# BEGIN SUBPROB

For each of several model instances, we train a model twice --- once
with the features as-is, and once with standardized features. In other
words:

```python
# Without standardization
model = ModelClass() # e.g. model = Lasso(alpha=10000)
model.fit(X, y)

# With standardization
model_std = make_pipeline(StandardScaler(), ModelClass())
model_std.fit(X, y)
```

We say a model is **standardization invariant** if its training mean
squared error is **guaranteed** to be the same, with or without
standardization. Which of the models below are standardization
invariant? **Select all** that apply.

[ ] `LinearRegression()`
[ ] `Ridge(alpha=10)`
[ ] `Lasso(alpha=1)`
[ ] `Lasso(alpha=10000)`
[ ] `DecisionTreeRegressor(max_depth=3)`
[ ] `KNeighborsRegressor(n_neighbors=5)`
[ ] `KNeighborsRegressor(n_neighbors=8)`

# BEGIN SOLUTION

**Answer**: `LinearRegression()` and `DecisionTreeRegressor(max_depth=3)`.

Let's look at each option individually.

- ✅ `LinearRegression()`: Notice that this model **does not** use regularization, i.e. the objective function we minimize to find this model's optimal parameter vector $\vec w^*$ does not use regularization. As we discussed in [Lecture 16](https://practicaldsc.org/resources/lectures/lec16/lec16-filled.html), standardizing the features of a (non-regularized) linear regression model does not change the model's predictions – and, hence, does not change its training MSE – instead, it only changes its coefficients. Hence, this model **is** standardization invariant.<br>
- ❌ `Ridge(alpha=10)`: This model **does** use regularization. To find this model's optimal parameters $\vec w^*$, we minimize the objective function
    $$
    R_\text{ridge}(\vec w) = \frac{1}{n} \lVert \vec y - X \vec w \rVert_2^2 + \lambda \sum_{j=1}^d w_j^2
    $$

    where $\lambda$ is the regularization hyperparameter. Standardizing the model's features changes the scales of the coefficients, which changes the size of the regularization term, $\lambda \sum_{j=1}^d w_j^2$. As a result, the model's parameters will all change **and** its predictions will change, meaning its MSE is not guaranteed to be the same. Hence, this model **is not** standardization invariant.
- ❌ `Lasso(alpha=1)`: Same logic as above.<br>
- ❌ `Lasso(alpha=10000)`: Same logic as above.<br>
- ✅ `DecisionTreeRegressor(max_depth=3)`: Recall, decision trees ask a sequence of yes/no questions using the features. If all of the features are standardized, the questions themselves can be rescaled in order for the tree to make the same predictions (which were originally optimal). For example, if pre-standardization, one of the questions was "is height < 5", post-standardization the same question could be "is height < -1.25", if a height of 5 was 1.25 standard deviations below the average height. So, this model **is** standardization invariant.<br>
- ❌ `KNeighborsRegressor(n_neighbors=5)`: Recall, $k$-nearest neighbors classifiers make predictions based on the distances between points. If the features are standardized, the distances between any two points will change, which changes the model's predictions. Hence, this model **is not** standardization invariant.<br>
- ❌ `KNeighborsRegressor(n_neighbors=8)`: Same logic as above.<br>

A good homework question to review here is [Homework 10, Question 2](https://github.com/practicaldsc/sp25/blob/main/homeworks/hw10/hw10.ipynb).

<average>68</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Consider the two model instances below.

```python
once = make_pipeline(StandardScaler(), ModelClass())
twice = make_pipeline(StandardScaler(), StandardScaler(), ModelClass())
```

True or False: As long as `ModelClass()` is a valid regression model in
`sklearn` that behaves deterministically\*, `once` and `twice` are
**guaranteed** to have the same training mean squared error.

( ) True
( ) False

<small>*By this, we mean that the model makes the same predictions every time it is fit on the same training set. Technically, not all models behave this way.*</small>

# BEGIN SOLUTION

**Answer**: True.

Suppose `x` is an array containing the information for a single feature. To standardize `x`, we can use the following code:

```python
x_std = (x - x.mean()) / x.std()
```

`x_std`, by definition, has a mean of 0 and standard deviation of 1. If we try and standardize `x_std`, the resulting array will be identical to `x_std`. In other words, if we run:

```python
x_std_again = (x_std - x_std.mean()) / x_std.std()
```

`x_std.mean()` is already 0, and `x_std.std()` is already 1, so `x_std_again` is being set to `(x_std - 0) / 1`, which is just `x_std`.

So, after we standardize a feature once, standardizing it again will not change the results.

<average>79</average>

# END SOLUTION

# END SUBPROB

# END PROB