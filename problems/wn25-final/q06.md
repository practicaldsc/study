# BEGIN PROB

Suppose we'd like to build a $L_1$-regularized (LASSO) regression model
to predict the width of a fish, given its height and weight. To choose a
value of $\lambda$ (the regularization hyperparameter) from the list
$[0.01, 0.1, 1, 10]$, we perform $k$-fold cross-validation with $k=4$.

# BEGIN SUBPROB

Consider the following optimal parameter vectors, each of which came
from minimizing $L_1$-regularized empirical risk using a different value
of $\lambda$.

(In each parameter vector, the 0th component represents the intercept
term, the 1st component represents the coefficient on height, and the
2nd component represents the coefficient on weight.)

$$\vec{w}_a^* = \begin{bmatrix} 2.6999 \\ 0.0105 \\ 0.0041 \end{bmatrix} \qquad
\vec{w}_b^* = \begin{bmatrix} 3.0674 \\ 0.0000 \\ 0.0034 \end{bmatrix} \qquad
\vec{w}_c^* = \begin{bmatrix} 2.0728 \\ 0.1236 \\ 0.0031 \end{bmatrix}$$

Which optimal parameter vector came from choosing $\lambda = 0.01$?

( ) $\vec w_a^*$
( ) $\vec w_b^*$
( ) $\vec w_c^*$

Which optimal parameter vector came from choosing $\lambda = 0.1$?

( ) $\vec w_a^*$
( ) $\vec w_b^*$
( ) $\vec w_c^*$

Which optimal parameter vector came from choosing $\lambda = 1$?

( ) $\vec w_a^*$
( ) $\vec w_b^*$
( ) $\vec w_c^*$

# BEGIN SOLUTION

**Answer**:

- $\lambda = 0.01$ corresponds to $\vec w_c^*$.
- $\lambda = 0.1$ corresponds to $\vec w_a^*$.
- $\lambda = 1$ corresponds to $\vec w_b^*$.

When performing regularization, as $\lambda$ increases, the model's optimal parameters (except for the intercept term!) decrease in magnitude, since we're penalizing the sizes of the model's parameters in the objective function:

$$R_\text{LASSO}(\vec w) = \frac{1}{n} \lVert \vec y - X \vec w \rVert^2 + \lambda \sum_{j=1}^d |w_i|$$

In the case of $L_1$ regularization (i.e. LASSO), as illustrated above, the model's optimal parameters shrink towards 0 exactly, for reasons discussed in [Lecture 19](https://practicaldsc.org/resources/lectures/lec19/lec19-filled.html). As the formula above dictates, we do **not** penalize the intercept term, $w_0$.

So, since $\lambda = 1$ has the largest value of $\lambda$ among the values we are considering, it corresponds to the optimal parameter vector in which the non-intercept parameters are the smallest, which is $\vec w_b^*$. $\lambda = 0.01$ has the smallest value of $\lambda$ among the values we are considering, so it corresponds to the optimal parameter vector that seems the least regularized, i.e. in which the non-intercept parameters are the largest, which is $\vec w_c^*$. $\lambda = 0.1$, by process of elimination, corresponds to $\vec w_a^*$.

<average>79</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose, just for this part, that:

-   Our entire dataset has $N$ rows.

-   To split the dataset of $N$ rows into training and test sets, we use
    `train_test_split` with `test_size=0.2`.

While performing 4-fold cross-validation, each time a model is trained,
90 points are used to train the model. What is the value of $N$?

( ) 100
( ) 108
( ) 120
( ) 150
( ) 180
( ) None of these

# BEGIN SOLUTION

**Answer**: 150.

- Remember, cross-validation only uses the training set, not the entire combined training and test set. Since we're performing a train-test split with `test_size=0.2`, 80% of the data is used for training, and 20% is used for testing, and so the training set size is $\frac{4}{5}N$.
- Since we're performing 4-fold cross-validation, each time a model is trained, $\frac{3}{4}$ of the training set is used to train the model. $\frac{3}{4}$ of the training set is $\frac{3}{4} \times \frac{4}{5}N = \frac{3}{5}N$.
- Since we're told that each time a model is trained, 90 points are used to train the model, we have $\frac{3}{5}N = 90$, and so $N = 90 \times \frac{5}{3} = 150$.

<average>44</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Average validation mean squared errors are shown in the table below.

:::: center
| | | | | |
|:--|:--|:--|:--|:--|
| | Fold 1 | Fold 2 | Fold 3 | Fold 4 |
| 位 = 0.01 | 15 | 9 | 12 | 12 |
| 位 = 0.1 | 12 | 18 | 6 | 12 |
| 位 = 1 | 3 | 12 | 15 | m |
| 位 = 10 | 18 | 6 | 3 | 9 |
::::

Given that $\lambda = 1$ is the hyperparameter with the best
cross-validation performance above, provide the best possible
upper-bound for a value of $m$. That is, find $M$ such that, as long as
$m < M$, $\lambda = 1$ is the best choice of $\lambda$. Give your answer
as a **constant with no variables**.

# BEGIN SOLUTION

**Answer**: $m < 6$.

We're told that $\lambda = 1$ is the best choice of $\lambda$, which means that its **average validation mean squared error** is the lowest of all the values of $\lambda$ we are considering.

We're given enough information to calculate the average validation mean squared error for each value of $\lambda$:

- $\lambda = 0.01$: $\frac{15 + 9 + 12 + 12}{4} = 12$
- $\lambda = 0.1$: $\frac{12 + 18 + 6 + 12}{4} = 12$
- $\lambda = 1$: $\frac{3 + 12 + 15 + m}{4} = \frac{30 + m}{4}$
- $\lambda = 10$: $\frac{18 + 6 + 3 + 9}{4} = 9$

Since $\lambda = 1$ has the lowest average validation mean squared error, we just need to find the range of possible values of $m$ that make $\frac{30 + m}{4} < 9$.

$$\frac{30 + m}{4} < 9$$

$$30 + m < 36$$

$$m < 6$$

<average>73</average>

# END SOLUTION

# END SUBPROB

# END PROB