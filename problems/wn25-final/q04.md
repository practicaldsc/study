# BEGIN PROB

Suppose we'd like to build a regression model to predict the width of a
fish, given its height, weight, and species. Consider the following two
possible approaches to building linear regression models:

-   **Approach 1**: One hot encode species using
    `OneHotEncoder(drop="first")`, use height and weight as-is, and fit
    a linear regression model.

-   **Approach 2**: Fit 7 separate sub-models, one for each unique value
    of species, where each sub-model is a linear regression model that
    uses height and weight only.

    -   For instance, one of the 7 sub-models would be for the Perch
        species; we'd query the training data to keep only the rows
        corresponding to the species Perch, then fit the Perch sub-model
        on just this subset of the training data.

    -   To predict the width of a new fish, we use the sub-model
        corresponding to the new fish's species.

    Assume that in **both** approaches, the linear regression models
    being considered include **intercept terms**.

# BEGIN SUBPROB

Suppose the mean squared error on the training set for approaches 1 and
2 are $\text{MSE}_1$ and $\text{MSE}_2$, respectively. Fill in the :

$$\displaystyle \text{MSE}_1 \:\:\: \boxed{???} \:\:\: \text{MSE}_2$$

( ) $\geq$
( ) $>$
( ) $=$
( ) $<$
( ) $\leq$
( ) Impossible to tell

# BEGIN SOLUTION

**Answer**: $\geq$.

The key to this problem is understanding the physical difference between the two approaches.

- Approach 1 **one hot encodes** species, meaning the model is of the form:

    $$
    \text{predicted width}_i = w_0 + w_1 \cdot \text{height}_i + w_2 \cdot \text{weight}_i + w_3 \left( \text{species}_i = {\text{Perch}} \right) + w_4 \left( \text{species}_i = {\text{Smelt}} \right) + ... + w_8 \left( \text{species}_i = {\text{Bream}} \right)
    $$

    Notice there are 6 species-specific parameters – this is because there are 7 species total, and we drop one of the species to avoid multicollinearity (through the use of `drop="first"` in the `OneHotEncoder`).
    
    Physically, this model looks like **7 different but _parallel_ planes in 3D** (if our axes are height, weight, and width). Each of these planes has the same slopes in the height and weight directions, but different $z$-intercepts.

- Approach 2 **fits 7 different planes in 3D, each of which has potentially different intercepts, but also potentially different slopes**.

This means that Approach 2 is more flexible than Approach 1. Suppose that the way to minimize mean squared error is to create 7 parallel planes – one per species – that each have the same height and weight coefficients. Approach 1 and Approach 2 can both do that. However, Approach 2 can do more than that – it can create planes that are not parallel, and that have different height and weight coefficients for each species, and will do so if it helps minimize mean squared error further.

So, Approach 2's mean squared error will always be less than or equal to Approach 1's mean squared error, which means $\text{MSE}_2 \leq \text{MSE}_1$, or $\text{MSE}_1 \boxed{\geq} \text{MSE}_2$ as the question intended.

This intuition was built through [Homework 7, Question 4](https://github.com/practicaldsc/sp25/blob/main/homeworks/hw07/hw07.ipynb), which introduced one hot encoding from a visual perspective.

<average>38</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose we visualize both models in 3 dimensions, where one axis
represents height, one axis represents weight, and one axis represents
predicted width.

Fill in the blanks: In the 3 dimensional space described above,

the fit model in **approach 1** looks like \_\_(i)\_\_ \_\_(ii)\_\_
\_\_(iii)\_\_,

while the fit model in **approach 2** looks like \_\_(iv)\_\_
\_\_(v)\_\_ \_\_(vi)\_\_.

- (i):

( ) 2
( ) 3
( ) 4
( ) 5
( ) 6
( ) 7

- (ii):

( ) identical
( ) possibly non-parallel
( ) orthogonal
( ) parallel

- (iii):

( ) lines
( ) planes
( ) points
( ) spheres
( ) vectors

- (iv):

( ) 2
( ) 3
( ) 4
( ) 5
( ) 6
( ) 7

- (v):

( ) identical
( ) possibly non-parallel
( ) orthogonal
( ) parallel

- (vi):

( ) lines
( ) planes
( ) points
( ) spheres
( ) vectors


# BEGIN SOLUTION

**Answer**: In the 3 dimensional space described above,

the fit model in **approach 1** looks like **7 parallel planes**,

while the fit model in **approach 2** looks like **7 possibly non-parallel planes**.

<average>64</average>

# END SOLUTION

# END SUBPROB

It is possible to construct a design matrix $X'$ such that approach 2 is
implemented as a single linear regression model, rather than 7 separate
linear regression models.

# BEGIN SUBPROB

How many columns are in the design matrix, $X'$?

# BEGIN SOLUTION

**Answer**: 21.

To implement approach 2 as a single linear regression model, we need to create two slopes – one for height and one for weight – plus one intercept term, **separately** for each of 7 features.

To better illustrate what we mean, let's imagine what $X'$ might look like. For example, let's suppose that:

- the first row in the training set corresponds to a Perch fish with height 50 and weight 25, and 
- the second row corresponds to a Smelt fish with height 2.43 and weight 13.4.

Then, $X'$ would look like:

$$X' = \begin{bmatrix} 1 & 50 & 25 & 0 & 0 & 0 & 0 & ... & 0\\ 0 & 0 & 0 & 1 & 2.43 & 13.4 & 0 & ... & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \end{bmatrix}_{n \times 21}$$

In the example above, the first three columns are "turned on" for Perch fish **only**; for non-Perch fish, their values are always 0. The second set of three columns are "turned on" for Smelt fish only, and so on and so forth.

<average>22</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

In 1-2 English sentences, describe how to create $X'$. Then, sketch an
example of how $X'$ might look, including at least two example rows, one
of which should correspond to a fish of the Perch species with a height
of 50 and weight of 25. Feel free to use ellipses, \..., in your answer.

# BEGIN SOLUTION

**Answer**: Answered above.

<average>51</average>

# END SOLUTION

# END SUBPROB

# END PROB