# BEGIN PROB

Suppose we'd like to build a regression model to predict the width of a
fish, given various features. Consider the six line graphs shown below.

<center><img src="../assets/images/wn25-final/curves.png" width=300></center>

In each part, select the graph that **best** represents the relationship
between model performance (drawn on the $y$-axis) and the provided
hyperparameter (drawn on the $x$-axis).

First, suppose we build a **$k$-nearest neighbors regression** model, as
seen in Homework 8.

# BEGIN SUBPROB

Which graph best represents **training mean squared error** ($y$-axis) vs. $k$ ($x$-axis)?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: B.

Small values of $k$ correspond to highly overfit $k$-nearest neighbors regression models. As we saw in Homework 8, a $k$-NN regression model with $k=1$ passes through every single training point, and has extremely low training set MSE (remember, MSE is low when model performance is "good" on the dataset we're using for evaluation). As $k$ increases, the model averages over more and more of the training set in order to make a prediction, and so training set MSE increases.

<average>100</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which graph best represents **test set mean squared error** vs. $k$?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: D.

A common pattern we've seen is that as model complexity increases, test set performance improves until a "sweet spot", and then worsens as the model starts to overfit. Here, as $k$ increases, model complexity **decreases**, though the graph of test set MSE would be the same as if increasing $k$ increases model complexity – it should decrease to a point, then increase again. This is precisely what happens in Option D.

<average>72</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which graph best represents **model variance** vs. $k$?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: A.

As explained in the answer to 5.1, as $k$ increases, model complexity decreases, and hence model variance decreases.

<average>48</average>

# END SOLUTION

# END SUBPROB

Now, suppose we build a **linear regression model with degree-$d$
polynomial features**.

# BEGIN SUBPROB

Which graph best represents **training mean squared error** vs. $d$?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Asnwer**: A.

As $d$ increases here, model complexity **increases**, so the model's tendency to overfit to training sets increases, and so training set MSE decreases.

<average>79</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which graph best represents **test set $R^2$** vs. $d$? *Hint: Recall
from Homework 8, $0 \leq R^2 \leq 1$, where larger values of $R^2$
indicate better predictions.*

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: E.

As $d$ increases – i.e. as model complexity increases – test set performance increases until a sweet spot, then decreases again once the model is too complex. The difference between $R^2$ and MSE is that large $R^2$ values correspond to better model performance, while large MSE values correspond to worse model performance. So, we're looking for a graph that first increases and then decreases vertically, which matches Option E. (Notice that E looks like an upside-down version of D, which would be the answer if we were asked for the graph that represents test set MSE vs. $d$.)

<average>74</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which graph best represents **model variance** vs. $d$?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: C.

As $d$ increases, model complexity increases, so model variance increases as well. There's no reason to believe model variance is bounded, as Option B indicates, hence Option C is the correct answer.

<average>64</average>

# END SOLUTION

# END SUBPROB

Finally, suppose we build a **ridge regression model with hyperparameter
$\lambda$**.

# BEGIN SUBPROB

Which graph best represents **model variance** vs. $\lambda$?

( ) A
( ) B
( ) C
( ) D
( ) E
( ) F

# BEGIN SOLUTION

**Answer**: A.

As $\lambda$ increases, model complexity decreases, and hence model variance decreases.

<average>74</average>

# END SOLUTION

# END SUBPROB

# END PROB