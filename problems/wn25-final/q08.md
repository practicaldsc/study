# BEGIN PROB

Suppose we build a classifier to predict whether a fish is of species
Bream (class 1) or not (class 0), given its height and/or weight.

Consider the training set of $n=40$ points below. Note that the 10
points in black correspond to class 1, and the 30 points correspond to
class 0. Assume there are no black points hidden under outlined points
and vice versa.

<center><img src="../assets/images/wn25-final/scatter-base.png" width=640></center>

# BEGIN SUBPROB

Suppose we use height **only** to predict species. In the 1-dimensional
feature space, is the training set linearly separable?

( ) Yes
( ) No

# BEGIN SOLUTION

**Answer**: No. 

There is no vertical line of the form $x = c$ that separates the black points from the outlined points.

<center><img src="../assets/images/wn25-final/8-1.png" width=640></center>

We're searching for a vertical line, because we're using height **only** to predict species, and with just one feature, we must consider if the data is separable using a line in a $d = 1$ dimensional plot, i.e. on a number line.

<average>90</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose we use **both** height and weight to predict species. In the
2-dimensional feature space, is the training set linearly separable?

( ) Yes
( ) No

# BEGIN SOLUTION

**Answer**: Yes.

Here, we're using $d = 2$ features, so we're searching for a line in the 2-dimensional feature space that separates the two classes perfectly.

And indeed, you can draw a sloped line of the form $y = mx + b$ – or more precisely, $\text{weight} = m \cdot \text{height} + b$ – that separates the black points from the outlined points. One such line is shown below:

<center><img src="../assets/images/wn25-final/8-2.png" width=640></center>

<average>92</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose we use **both** height and weight to predict species. What is
**minimum** depth $d$ required for a decision tree classifier to achieve
a training accuracy of 100%?

( ) 1
( ) 2
( ) 3
( ) 4
( ) 5
( ) 10
( ) 40

# BEGIN SOLUTION

**Answer**: 2.

See the decision boundary below:

<center><img src="../assets/images/wn25-final/8-3-db.png" width=640></center>

The resulting fit decision tree would be:

<center><img src="../assets/images/wn25-final/8-3-tree.png" width=640></center>

This tree only needs to ask a maximum of 2 questions to classify any point in the training set correctly.

<average>64</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose we use **both** height and weight to predict species. If we use
a $k$-nearest neighbors classifier with $k=1$, which class would be
predicted for a fish with height 20 and weight 1250?

( ) Class 1 (Bream)
( ) Class 0 (non-Bream)

# BEGIN SOLUTION

**Answer**: Class 0 (non-Bream).

Given that $k = 1$, we classify this new fish as having the same class as one nearest neighbor to it in the training set. The nearest neighbor to this new fish is the fish with height 10 and weight 1250, which is a non-Bream fish. This result is a little counterintuitive at first glance – it _looks_ like the closest neighbor is a black point (Bream) – but pay attention to the scale of the axes!

<center><img src="../assets/images/wn25-final/8-4.png" width=640></center>

<average>14</average>

# END SOLUTION

# END SUBPROB

For your convenience, we show the training set of $n=40$ points again
below, in which 10 points belong to class 1.

<center><img src="../assets/images/wn25-final/scatter-base.png" width=640></center>

# BEGIN SUBPROB

Suppose we use height ($x_i$) **only** to predict species. If we use
logistic regression without regularization, which option best describes
the fit model, $P(y_i = 1 | x_i)$?

( ) $P(y_i = 1 | x_i) = \sigma \left(-15 - \frac{2}{3}x_i \right)$
( ) $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{2}{3}x_i \right)$
( ) $P(y_i = 1 | x_i) = \sigma \left(-20 + \frac{2}{3}x_i \right)$
( ) $P(y_i = 1 | x_i) = \sigma \left(-15 - \frac{5}{4}x_i \right)$
( ) $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{5}{4}x_i \right)$
( ) $P(y_i = 1 | x_i) = \sigma \left(-20 + \frac{5}{4}x_i \right)$

# BEGIN SOLUTION

**Answer**: $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{5}{4}x_i \right)$

Firstly, we can rule out any option with a negative coefficient on $x_i$. This is because the frequency of Bream fish increases as height increases, so the probability of a Bream fish should increase as height increases. (Remember that $\sigma(t)$ is an increasing function of $t$.)

When deciding between the remaining options, we can use the fact that $\sigma(0) = \frac{1}{2}$. Intuitively, the predicted probability that a fish is a Bream fish should be $\frac{1}{2}$ in the "middle" of the data, when we are seeing equally many Bream and non-Bream fish. This roughly occurs around a height of 12.

<center><img src="../assets/images/wn25-final/8-5.png" width=640></center>

If we consider the four options that have positive coefficients on $x_i$ and solve for $x_i$ when $P(y_i = 1 | x_i) = \frac{1}{2}$, we get the following:

- $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{2}{3}x_i \right) \implies x_i = \frac{45}{2}$
- $P(y_i = 1 | x_i) = \sigma \left(-20 + \frac{2}{3}x_i \right) \implies x_i = 30$
- $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{5}{4}x_i \right) \implies x_i = \boxed{12}$
- $P(y_i = 1 | x_i) = \sigma \left(-20 + \frac{5}{4}x_i \right) \implies x_i = 16$

So, we choose $P(y_i = 1 | x_i) = \sigma \left(-15 + \frac{5}{4}x_i \right)$.

<average>64</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose we use height ($x_i$) **only** to predict species. If we use
$L_2$-regularized logistic regression with a regularization
hyperparameter of $\lambda = 10^{10}$, what is the value of $w_0^*$ in
the fit model $P(y_i = 1 | x_i) = \sigma(w_0^* + w_1^*x_i)$?

( ) $3$
( ) $1 / 3$
( ) $4$
( ) $1 / 4$
( ) $\log \left( 3 \right)$
( ) $\log \left( 1 / 3 \right)$
( ) $\log \left( 4 \right)$
( ) $\log \left( 1 / 4 \right)$

# BEGIN SOLUTION

**Answer**: $\log \left( 1 / 3 \right)$.

If we let $\lambda \rightarrow \infty$ in the regularized objective function, the value of $w_1^*$ is forced to 0. However, the value of $w_0^*$ is not constrained, since we don't regularize the intercept term. Instead, our model will be of the form:

$$P(y_i = 1 | x_i) = \sigma(w_0^*)$$

Independent of the value of $x_i$, the probability that a fish is of species Bream is $\frac{1}{4}$, since 10 of the 40 points in the training set correspond to Bream fish. So, we have:

$$\frac{1}{4} = \sigma(w_0^*)$$

Now, we must solve for $w_0^*$ in the equation above. To do so, we use the fact that if $y = \sigma(x)$, then $x = \log \left( \frac{y}{1 - y} \right)$. So, we have:

$$w_0^* = \log \left( \frac{\frac{1}{4}}{1 - \frac{1}{4}} \right) = \log \left( 1 / 3 \right)$$

So, the value of $w_0^*$ is $\boxed{\log \left( 1 / 3 \right)}$.

<average>34</average>

# END SOLUTION

# END SUBPROB

For the rest of this question, suppose we use **both** height and weight
to predict species. Suppose we use logistic regression --- possibly with
regularization --- and choose a classification threshold of $T$, where
$0 \leq T \leq 1$. The resulting decision boundary is shown below, along
with the same training set as before (which has $n=40$ points, 10 of
which belong to class 1).

<center><img src="../assets/images/wn25-final/scatter-with-boundary.png" width=640></center>

In the region above the line, shaded gray, our classifier predicts class
1 (Bream); in the region below the line, our classifier predicts class 0
(non-Bream).

# BEGIN SUBPROB

Was regularization used when fitting the logistic regression model whose
decision boundary is shown above?

( ) Yes, regularization was used
( ) No, regularization was not used

# BEGIN SOLUTION

**Answer**: Yes, regularization was used.

If regularization wasn't used, the decision boundary would look much closer to the line shown in Problem 8.2's solution, which has very high training accuracy. This line is much worse than that one, and so it must have been regularized. (Some regularization is necessary since the dataset is linearly separable, but in practice to fit this data, we'd have used a much smaller regularization hyperparameter than the one that was used here.)

<average>47</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

What are the precision and recall of the classifier above? Give your
answers as simplified fractions. Answers that "swap\" precision and
recall will not be given any credit.

# BEGIN SOLUTION

**Answer**: Precision = $\frac{3}{7}$, Recall = $\frac{3}{5}$.

Recall (no pun intended), we have that:

$$\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$$

$$\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$$

Here:
- A true positive is a Bream fish that was correctly classified as a Bream fish (a black point in the shaded area), of which there are 6.
- A false positive is a non-Bream fish that was incorrectly classified as a Bream fish (an outlined point in the shaded area), of which there are 8.
- A false negative is a Bream fish that was incorrectly classified as a non-Bream fish (a black point in the non-shaded area), of which there are 4.

So, we have:

$$\text{Precision} = \frac{6}{6 + 8} = \frac{6}{14} = \frac{3}{7}$$

$$\text{Recall} = \frac{6}{6 + 4} = \frac{6}{10} = \frac{3}{5}$$

<center><img src="../assets/images/wn25-final/8-8.png" width=640></center>

<average>75</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose that each time we accidentally misclassify a fish's species as
Bream, we must pay a fine to the local aquarium. Given this fact alone,
is it more important for our classifier to have high precision or high
recall?

( ) High precision
( ) High recall

# BEGIN SOLUTION

**Answer**: High precision.

Here, we're being penalized for misclassifying a Bream fish as a non-Bream fish. So, we want to minimize the number of false positives. Since precision decreases as the number of false positives increases, we want to maximize precision.

<average>88</average>

# END SOLUTION

# END SUBPROB

# END PROB