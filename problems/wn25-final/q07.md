# BEGIN PROB

Let $\vec x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$. Consider the
function $f(\vec x) = (x_1^2 + x_2 - 3)^2 + (x_1 + x_2^2 - 4)^2$.

# BEGIN SUBPROB

$\nabla f(\vec x)$, the gradient of $f$, can be written in the form
$\boxed{\nabla f(\vec x) = A \vec g}$, where
$A \in \mathbb{R}^{2 \times 2}$ and $\vec g \in \mathbb{R}^2$. Fill in
the blanks to complete the definitions of $A$ and $\vec g$. All blanks
should be filled with expressions involving $x_1$, $x_2$, and/or
constants.

$$A = \begin{bmatrix} 2x_1 & \_\_(\text{i})\_\_ \\ 1 & \_\_(\text{ii})\_\_ \end{bmatrix} \qquad \qquad \vec g = 2\begin{bmatrix} \_\_(\text{iii})\_\_  \\ \_\_(\text{iv})\_\_ \end{bmatrix}$$

# BEGIN SOLUTION

**Answer**:

$$A = \begin{bmatrix} 2x_1 & 1 \\ 1 & 2x_2 \end{bmatrix} \qquad \qquad \vec g = 2\begin{bmatrix} x_1^2 + x_2 - 3 \\ x_1 + x_2^2 - 4 \end{bmatrix}$$

<average>75</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

We'd like to use gradient descent to minimize $f$. We choose an initial
guess of $\vec x^{(0)} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and
learning rate/step size $\beta$. Given that
$A = \begin{bmatrix} 2 & 1 \\ 1 & 0 \end{bmatrix}$ and
$\vec g = \begin{bmatrix} -4 \\ -6 \end{bmatrix}$ when evaluated on
$\vec x^{(0)}$, perform one iteration of gradient descent. In other
words, what is $\vec x^{(1)}$?

Show your work, and put a $\boxed{\text{box}}$ around your final answer,
which should be a vector with two components, both of which are
expressions involving $\beta$ and/or constants, but no other variables
(i.e. $x$ should not appear in your answer).

# BEGIN SOLUTION

**Answer**: $$\vec x^{(1)} = \boxed{\begin{bmatrix} 1 + 14\beta \\ 4\beta \end{bmatrix}}$$

$$\begin{aligned}
\vec x^{(1)} &= \vec x^{(0)} - \beta \nabla f(\vec x^{(0)}) \\
&= \begin{bmatrix} 1 \\ 0 \end{bmatrix} - \beta \begin{bmatrix} 2 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} -4 \\ -6 \end{bmatrix} \\
&= \begin{bmatrix} 1 \\ 0 \end{bmatrix} - \beta \begin{bmatrix} -8 - 6 \\ -4 \end{bmatrix} \\
&= \begin{bmatrix} 1 \\ 0 \end{bmatrix} + \beta \begin{bmatrix} 8 + 6 \\ 4 \end{bmatrix} \\
&= \begin{bmatrix} 1 + 14\beta \\ 4\beta \end{bmatrix}
\end{aligned}$$

<average>75</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Suppose $Q(\vec x)$ is a convex function. True or False: Given that
gradient descent converges to the global minimum of $Q$ in $T$
iterations, it is guaranteed that:

$$\lVert \vec x^{(1)} - \vec x^{(0)} \rVert > \lVert \vec x^{(t+1)} - \vec x^{(t)} \rVert, \:\:\: \text{for } t = 1, 2, ..., T-1$$

( ) True
( ) False

# BEGIN SOLUTION

**Answer**: False.

In English, the statement says that the distance between the first and second iteration is greater than the distance between any other consecutive iterations, for all iterations up to $T-1$. In other words, the claim is that the first "jump" is the largest – not in terms of how much the output of the function $Q$ changes, but in terms of how much the guess for $\vec w^*$ changes.

This is not guaranteed to be true – our first iteration may happen to be very small, and a subsequent iteration may happen to be very large, and gradient descent could still converge.

<average>39</average>

# END SOLUTION

# END SUBPROB

# END PROB