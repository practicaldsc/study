# BEGIN PROB

Suppose $A \in \mathbb{R}^{n \times d}$ is a matrix,
$\vec b \in \mathbb{R}^n$ is a vector, $\theta$ is a **negative**
number, and that $\vec v^*$ is **a** vector that minimizes
$\lVert \vec b - A \vec v \rVert^2$. In other words:
$$\vec v^* = \underset{\vec v}{\text{argmin}} \lVert \vec b - A \vec v \rVert$$

Furthermore, suppose that one of the columns in $A$ is
$\vec \theta = \begin{bmatrix} \theta \\ \theta \\ \vdots \\ \theta\end{bmatrix}$.

What is the value of $\vec \theta^T (\vec b - A \vec v^*)$?

( ) $0$
( ) $\vec 0$
( ) $1$
( ) $\vec 1$
( ) $\theta$
( ) $\vec \theta$
( ) None of these

# BEGIN SOLUTION

**Answer**: 0.

First, note that the expression $\vec \theta^T (\vec b - A \vec v^*)$ is computing the dot product of $\vec \theta$ and $\vec b - A \vec v^*$, which means the result must be a scalar. This rules $\vec 0$, $\vec 1$, and $\vec \theta$ as possible options.

One of the big ideas in [Lecture 14: Regression using Linear Algebra](https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf#page=24) is that the parameter vector $\vec v^*$ that minimizes $$\lVert \vec b - A \vec v \rVert^2$$ is chosen in order to guarantee that the columns of $A$ are orthogonal to the error vector, $\vec b - A \vec v^*$. Since $\vec \theta = \begin{bmatrix} \theta \\ \theta \\ \vdots \\ \theta\end{bmatrix}$ is a column in $A$, we know that $\vec \theta$ must be orthogonal to $\vec b - A \vec v^*$, meaning the dot product of the two vectors in question must be 0.

<average>57</average>

# END SOLUTION

Note: There was originally another part of this problem, but we've removed it, since its answer was not well defined.

# END PROB