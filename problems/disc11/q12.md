# BEGIN PROB

Suppose we fit five different classifiers that predict whether a product
is designed for sensitive skin, given its price and number of
ingredients. In the five decision boundaries below, the gray-shaded
regions represent areas in which the classifier would predict that the
product **is** designed for sensitive skin (i.e. predict class 1).

<figure style="display: flex; flex-wrap: wrap; gap: 10px;">
    <div class="minipage">
        <img src="../assets/images/fa24-final/db1.png" width="300"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db2.png" width="300"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db3.png" width="300"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db4.png" width="300"/>
    </div>
    <div class="minipage">
        <img src="../assets/images/fa24-final/db5.png" width="300"/>
    </div>
</figure>

# BEGIN SUBPROB

Which model does Decision Boundary 1 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: Logistic regression  

Logistic regression is a linear classification technique, meaning it creates a single linear decision boundary between classes. Among the five decision boundaries, only Decision Boundary 1 is linear, making it the correct answer.


# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 2 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: Decision tree with $\text{max depth} = 15$

We know that Decision Boundaries 2 and 5 are decision trees, since the decision boundaries are parallel to the axes. Decision boundaries for decision trees are parallel to axes because the tree splits the feature space based on one feature at a time, creating partitions aligned with that feature's axis (e.g., "Price â‰¤ 100" results in a vertical split).

Decision Boundary 2 is more complicated than Decision Boundary 5, so we can assume there are more levels. 


# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 3 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: $k$-nearest neighbors with $k = 3$

$k$-nearest neighbors decision boundaries don't follow straight lines like logistic regression or decision trees. Thus, boundaries 3 and 4 are $k$-nearest neighbors. 

A lower value of $k$ means the model is more sensitive to local variations in the data. Since Decision Boundary 3 has more complex and irregular regions than Decision Boundary 4, it corresponds to $k = 3$, where the model closely follows the training data.

For example, if in our training data we had 3 points in the group represented by the light-shaded region at $(300, 150)$, the decision boundary for $k$-nearest neighbors with $k = 3$ would be light-shaded at $(300, 150)$, whereas it may be dark-shaded for $k$-nearest neighbors with $k = 100$. 




# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Which model does Decision Boundary 4 correspond to?

( ) $k$-nearest neighbors with $k = 3$
( ) $k$-nearest neighbors with $k = 100$
( ) Decision tree with $\text{max depth} = 3$
( ) Decision tree with $\text{max depth} = 15$
( ) Logistic regression

# BEGIN SOLUTION
**Answer**: $k$-nearest neighbors with $k = 100$

$k$-nearest neighbors decision boundaries don't follow straight lines like logistic regression or decision trees. Thus, boundaries 3 and 4 are $k$-nearest neighbors. 

$k$-nearest neighbors decision boundaries adapt to the training data, but increasing $k$ makes the model less sensitive to local variations.  

Since Decision Boundary 4 is smoother and has fewer regions than Decision Boundary 3, it corresponds to $k = 100$, where predictions are averaged over a larger number of neighbors, making the boundary less sensitive to individual data points.



# END SOLUTION

# END SUBPROB

# END PROB